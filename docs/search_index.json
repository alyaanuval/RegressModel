[
["index.html", "Online Tutorial on Regression Modeling with Actuarial and Financial Applications Preface", " Online Tutorial on Regression Modeling with Actuarial and Financial Applications Edward W. (Jed) Frees, University of Wisconsin-Madison Preface Date: 17 October 2018 About Regression Modeling Statistical techniques can be used to address new situations. This is important in a rapidly evolving risk management world. Analysts with a strong analytical background understand that a large data set can represent a treasure trove of information to be mined and can yield a strong competitive advantage. This book and online tutorial provides budding analysts with a foundation in multiple reression. Viewers will learn about these statistical techniques using data on the demand for insurance, healthcare expenditures, and other applications. Although no specific knowledge of actuarial or risk management is presumed, the approach introduces applications in which statistical techniques can be used to analyze real data of interest. Resources This tutorial is based on the book Regression Modeling with Actuarial and Financial Applications. For resources associated with the book, please visit the Regression Modeling book web site. For advanced regression applications in insurance, you may be interested in the series, Predictive Modeling Applications in Actuarial Science. Sample code and data for the series are available at series website. An earlier version of this tutorial, a Short Course constructed for Indonesian actuaries, uses the Datacamp learning platform. Tutorial Description This online tutorial is designed to guide you through the foundations of regession with applications in actuarial science. Anticipated completion time is approximately six hours. The tutorial assumes that you are familiar with the foundations in the statistical software R, such as Datacamp’s Introduction to R. General Layout. There are five chapters in this tutorial that summarize the foundations of multiple linear regression. Each chapter is subdivided into several sections. At the beginning of each section is a short video, typically 4-8 minutes, that summarizes the section key learning outcomes. Following the video, you can see more details about the underlying R code for the analysis presented in the video. Role of Exercises. Following each video, there are one or two exercises that allow you to practice skills to make sure that you fully grasp the learning outcomes. The exercises are implented using an online learning platfor provided by Datacamp so that you need not install R. Feedback is programmed into the exercises so that you will learn a lot by making mistakes! You will be pacing yourself, so always feel free to reveal the answers by hitting the Solution tab. Remember, going through quickly is not equivalent to learning deeply. Use this tool to enhance your understanding of one of the foundations of data science, regression analysis. Welcome to the Tutorial Video In this video, you learn how to: Describe regression briefly, i.e., in a nutshell Explain Galton’s height example as a regression application Video Overhead Show Overhead A. Galton’s 1885 Regression Data \\[ \\small{\\begin{array}{l|ccccccccccc|c} \\hline \\text{Height of }&amp; &amp; &amp; &amp; &amp; &amp; &amp; &amp; &amp; &amp; &amp; &amp; \\\\ \\text{adult child }&amp; &amp; &amp; &amp; &amp; &amp; &amp; &amp; &amp; &amp; &amp; &amp; \\\\ \\text{in inches }&amp; &lt;64.0 &amp; 64.5 &amp; 65.5 &amp; 66.5 &amp; 67.5 &amp; 68.5 &amp; 69.5 &amp; 70.5 &amp; 71.5 &amp; 72.5 &amp; &gt;73.0 &amp; \\text{Totals} \\\\ \\hline &gt;73.7 &amp; - &amp; - &amp; - &amp; - &amp; - &amp; - &amp; 5 &amp; 3 &amp; 2 &amp; 4 &amp; - &amp; 14 \\\\ 73.2 &amp; - &amp; - &amp; - &amp; - &amp; - &amp; 3 &amp; 4 &amp; 3 &amp; 2 &amp; 2 &amp; 3 &amp; 17 \\\\ 72.2 &amp; - &amp; - &amp; 1 &amp; - &amp; 4 &amp; 4 &amp; 11 &amp; 4 &amp; 9 &amp; 7 &amp; 1 &amp; 41 \\\\ 71.2 &amp; - &amp; - &amp; 2 &amp; - &amp; 11 &amp; 18 &amp; 20 &amp; 7 &amp; 4 &amp; 2 &amp; - &amp; 64 \\\\ 70.2 &amp; - &amp; - &amp; 5 &amp; 4 &amp; 19 &amp; 21 &amp; 25 &amp; 14 &amp; 10 &amp; 1 &amp; - &amp; 99 \\\\ 69.2 &amp; 1 &amp; 2 &amp; 7 &amp; 13 &amp; 38 &amp; 48 &amp; 33 &amp; 18 &amp; 5 &amp; 2 &amp; - &amp; 167 \\\\ 68.2 &amp; 1 &amp; - &amp; 7 &amp; 14 &amp; 28 &amp; 34 &amp; 20 &amp; 12 &amp; 3 &amp; 1 &amp; - &amp; 120 \\\\ 67.2 &amp; 2 &amp; 5 &amp; 11 &amp; 17 &amp; 38 &amp; 31 &amp; 27 &amp; 3 &amp; 4 &amp; - &amp; - &amp; 138 \\\\ 66.2 &amp; 2 &amp; 5 &amp; 11 &amp; 17 &amp; 36 &amp; 25 &amp; 17 &amp; 1 &amp; 3 &amp; - &amp; - &amp; 117 \\\\ 65.2 &amp; 1 &amp; 1 &amp; 7 &amp; 2 &amp; 15 &amp; 16 &amp; 4 &amp; 1 &amp; 1 &amp; - &amp; - &amp; 48 \\\\ 64.2 &amp; 4 &amp; 4 &amp; 5 &amp; 5 &amp; 14 &amp; 11 &amp; 16 &amp; - &amp; - &amp; - &amp; - &amp; 59 \\\\ 63.2 &amp; 2 &amp; 4 &amp; 9 &amp; 3 &amp; 5 &amp; 7 &amp; 1 &amp; 1 &amp; - &amp; - &amp; - &amp; 32 \\\\ 62.2 &amp; - &amp; 1 &amp; - &amp; 3 &amp; 3 &amp; - &amp; - &amp; - &amp; - &amp; - &amp; - &amp; 7 \\\\ &lt;61.2 &amp; 1 &amp; 1 &amp; 1 &amp; - &amp; - &amp; 1 &amp; - &amp; 1 &amp; - &amp; - &amp; - &amp; 5 \\\\ \\hline \\text{Totals }&amp; 14 &amp; 23 &amp; 66 &amp; 78 &amp; 211 &amp; 219 &amp; 183 &amp; 68 &amp; 43 &amp; 19 &amp; 4 &amp; 928 \\\\ \\hline \\end{array}} \\] Show Overhead B. Supporting R Code # Reformat Data Set #heights &lt;- read.csv(&quot;CSVData\\\\GaltonFamily.csv&quot;,header = TRUE) heights &lt;- read.csv(&quot;https://assets.datacamp.com/production/repositories/2610/datasets/c85ede6c205d22049e766bd08956b225c576255b/galton_height.csv&quot;, header = TRUE) str(heights) head(heights) heights$child_ht &lt;- heights$CHILDC heights$parent_ht &lt;- heights$PARENTC heights2 &lt;- heights[c(&quot;child_ht&quot;,&quot;parent_ht&quot;)] #heights &lt;- read.csv(&quot;CSVData\\\\galton_height.csv&quot;,header = TRUE) heights &lt;- read.csv(&quot;https://assets.datacamp.com/production/repositories/2610/datasets/c85ede6c205d22049e766bd08956b225c576255b/galton_height.csv&quot;, header = TRUE) plot(jitter(heights$parent_ht),jitter(heights$child_ht), ylim = c(60,80), xlim = c(60,80), ylab = &quot;height of child&quot;, xlab = &quot;height of parents&quot;) abline(lm(heights$child_ht~heights$parent_ht)) abline(0,1,col = &quot;red&quot;, lty=2) summary(lm(heights$child_ht~heights$parent_ht)) Call: lm(formula = heights$child_ht ~ heights$parent_ht) Residuals: Min 1Q Median 3Q Max -8.2577 -1.4280 0.1323 1.5720 5.7918 Coefficients: Estimate Std. Error t value Pr(&gt;|t|) (Intercept) 25.84856 2.69009 9.609 &lt;2e-16 *** heights$parent_ht 0.60992 0.03882 15.710 &lt;2e-16 *** --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Residual standard error: 2.26 on 926 degrees of freedom Multiple R-squared: 0.2104, Adjusted R-squared: 0.2096 F-statistic: 246.8 on 1 and 926 DF, p-value: &lt; 2.2e-16 "],
["regression-and-the-normal-distribution.html", "Chapter 1 Regression and the Normal Distribution 1.1 Fitting a normal distribution 1.2 Visualizing distributions 1.3 Summarizing distributions 1.4 Transformations", " Chapter 1 Regression and the Normal Distribution Chapter description Regression analysis is a statistical method that is widely used in many fields of study, with actuarial science being no exception. This chapter introduces the role of the normal distribution in regression and the use of logarithmic transformations in specifying regression relationships. 1.1 Fitting a normal distribution In this section, you learn how to: Calculate and interpret two basic summary statistics Fit a data set to a normal curve Calculate probabilities under a standard normal curve 1.1.1 Video Video Overhead Details Show Overhead A Details. Description of the data To illustrate a data set that can be analyzed using regression methods, we consider some data included in Galton’s 1885 paper. These data include the heights of 928 adult children (child_ht), together with an index of their parents’ height (parent_ht). Here, all female heights were multiplied by 1.08, and the index was created by taking the average of the father’s height and rescaled mother’s height. Galton was aware that the parents’ and the adult child’s height could each be adequately approximated by a normal curve. In developing regression analysis, he provided a single model for the joint distribution of heights. heights &lt;- read.csv(&quot;CSVData\\\\galton_height.csv&quot;, header = TRUE) #heights &lt;- read.csv(&quot;https://assets.datacamp.com/production/repositories/2610/datasets/c85ede6c205d22049e766bd08956b225c576255b/galton_height.csv&quot;, header = TRUE) plot(jitter(heights$parent_ht),jitter(heights$child_ht), ylim = c(60,80), xlim = c(60,80), ylab = &quot;height of child&quot;, xlab = &quot;height of parents&quot;) abline(lm(heights$child_ht~heights$parent_ht)) abline(0,1,col = &quot;red&quot;) Show Overhead B Details. Read and examine data structure The data has already been read into a dataset called heights. Examine the structure of the data with the function str() and use the head() command to looks at the first few records. heights &lt;- read.csv(&quot;CSVData\\\\galton_height.csv&quot;,header = TRUE) #heights &lt;- read.csv(&quot;https://assets.datacamp.com/production/repositories/2610/datasets/c85ede6c205d22049e766bd08956b225c576255b/galton_height.csv&quot;, header = TRUE) str(heights) head(heights) &#39;data.frame&#39;: 928 obs. of 2 variables: $ child_ht : num 72.2 73.2 73.2 73.2 68.2 ... $ parent_ht: num 74.5 74.5 74.5 74.5 73.5 73.5 73.5 73.5 73.5 73.5 ... child_ht parent_ht 1 72.2 74.5 2 73.2 74.5 3 73.2 74.5 4 73.2 74.5 5 68.2 73.5 6 69.2 73.5 Show Overhead C Details. Summary stats for parents’ height Next, examine the distribution of the child’s height and then examine the distribution of the parents height. ht_par &lt;- heights$parent_ht hist(ht_par) mean(ht_par) sd(ht_par) [1] 69.26293 [1] 1.912274 Show Overhead D. Fit a normal curve to parents’ height details (mparent &lt;- mean(ht_par)) (sdparent &lt;- sd(ht_par)) x &lt;- seq(60, 80,by = 0.1) hist(ht_par, freq = FALSE) lines(x, dnorm(x, mean = mparent, sd = sdparent), col = &quot;blue&quot;) [1] 69.26293 [1] 1.912274 Show Overhead E Details. Use the normal approximation to determine the probability of the height of tall parents TallHeight &lt;- 72 pnorm(TallHeight, mean = mparent, sd = sdparent) pnorm(72, mean = mean(ht_par), sd = sd(ht_par)) (StdUnitsTallHeight &lt;- (TallHeight - mparent)/sdparent) pnorm(StdUnitsTallHeight, mean = 0, sd = 1) [1] 0.9238302 [1] 0.9238302 [1] 1.431317 [1] 0.9238302 1.1.2 Exercise. Fitting Galton’s height data Assignment Text The Galton data has already been read into a dataframe called heights. These data include the heights of 928 adult children child_ht, together with an index of their parents’ height parent_ht. The video explored the distribution of the parents’ height; in this assignment, we investigate the distribution of the heights of the adult children. Instructions Define the height of an adult child as a global variable Use the function mean() to calculate the mean and the function sd() to calculate the standard deviation Use the normal approximation and the function pnorm() determine the probability that an adult child’s height is less than 72 inches Hint. Remember that we can reference a variable, say var, from a data set such as heights, as heights$var. eyJsYW5ndWFnZSI6InIiLCJwcmVfZXhlcmNpc2VfY29kZSI6IiNoZWlnaHRzIDwtIHJlYWQuY3N2KFwiQ1NWRGF0YVxcXFxnYWx0b25faGVpZ2h0LmNzdlwiLGhlYWRlciA9IFRSVUUpXG5oZWlnaHRzIDwtIHJlYWQuY3N2KFwiaHR0cHM6Ly9hc3NldHMuZGF0YWNhbXAuY29tL3Byb2R1Y3Rpb24vcmVwb3NpdG9yaWVzLzI2MTAvZGF0YXNldHMvYzg1ZWRlNmMyMDVkMjIwNDllNzY2YmQwODk1NmIyMjVjNTc2MjU1Yi9nYWx0b25faGVpZ2h0LmNzdlwiLCBoZWFkZXIgPSBUUlVFKSIsInNhbXBsZSI6IiNEZWZpbmUgdGhlIGdsb2JhbCB2YXJpYWJsZVxuaHRfY2hpbGQgPC0gX19fXG5cbiNDYWxjdWxhdGUgdGhlIG1lYW4gaGVpZ2h0XG5tY2hpbGQgPC0gX19fXG5tY2hpbGRcblxuI0NhbGN1bGF0ZSB0aGUgc3RhbmRhcmQgZGV2aWF0aW9uIG9mIGhlaWdodHNcbnNkY2hpbGQgPC0gX19fXG5zZGNoaWxkXG5cbiNEZXRlcm1pbmUgdGhlIHByb2JhYmlsaXR5IHRoYXQgdGhlIGhlaWdodCBpcyBsZXNzIHRoYW4gNzJcbl9fXyg3MiwgbWVhbj1tY2hpbGQsIHNkPXNkY2hpbGQpIiwic29sdXRpb24iOiIjIFNvbHV0aW9uXG5odF9jaGlsZCA8LSBoZWlnaHRzJGNoaWxkX2h0XG5tY2hpbGQgPC0gbWVhbihodF9jaGlsZClcbnNkY2hpbGQgPC0gc2QoaHRfY2hpbGQpXG5tY2hpbGRcbnNkY2hpbGRcbnBub3JtKDcyLCBtZWFuID0gbWNoaWxkLCBzZCA9IHNkY2hpbGQpIiwic2N0IjoiZXgoKSAlPiUgY2hlY2tfb2JqZWN0KFwiaHRfY2hpbGRcIix1bmRlZmluZWRfbXNnPVwiTWFrZSBzdXJlIHlvdSBhc3NpZ24gdGhlIGNoaWxkcmVuJ3MgaGlnaHQgdG8gaHRfY2hpbGRcIikgJT4lIGNoZWNrX2VxdWFsKGluY29ycmVjdF9tc2c9XCJSZW1lbWJlciB0aGF0IGluIG9yZGVyIHRvIGNhbGwgYSBzcGVjaWZpYyBjb2x1bW4gZnJvbSBhIGRhdGFmcmFtZSwgdXNlIHRoZSAkIG9wZXJhdG9yXCIpXG5leCgpICU+JSBjaGVja19vYmplY3QoXCJtY2hpbGRcIikgICU+JSBjaGVja19lcXVhbCgpXG5leCgpICU+JSBjaGVja19vYmplY3QoXCJzZGNoaWxkXCIpICU+JSBjaGVja19lcXVhbCgpXG5leCgpICU+JSBjaGVja19vYmplY3QoXCJwclwiKSAlPiUgY2hlY2tfZXF1YWwoKVxuc3VjY2Vzc19tc2coXCJFeGNlbGxlbnQhIFdpdGggdGhpcyBwcm9jZWR1cmUsIHlvdSBjYW4gbm93IGNhbGN1bGF0ZSBwcm9iYWJpbGl0aWVzIGZvciBhbnkgZGlzdHJpYnV0aW9uIHVzaW5nIGEgbm9ybWFsIGN1cnZlIGFwcHJveGltYXRpb24uXCIpIn0= 1.1.3 Exercise. Visualizing child’s height distribution Assignment Text As in the prior exercise, from the Galton dataset heights, the heights of 928 adult children have been used to create a global variable called ht_child. We also have basic summary statistics, the mean height mchild and the standard deviation of heights in sdchild. In this exercise, we explore the fit of the normal curve to this distribution. Instructions To visualize the distribution, use the function hist() to calculate the histogram. Use the freq = FALSE option to give a histogram with proportions instead of counts. Use the function seq() to determine a sequence that can be used for plotting. Then, with the function lines(), superimpose a normal curve on the histogram Determine the probability that a child’s height is greater than 72 inches Hint 1. Use the function dnorm() to calculate the normal density, similar to the cumulative probabilites that you calculated using pnorm() Hint 2. To calculate probabilities greater that an amount, simply use 1 minus the cumulative probability Pre-exercise code eyJsYW5ndWFnZSI6InIiLCJwcmVfZXhlcmNpc2VfY29kZSI6IiNoZWlnaHRzIDwtIHJlYWQuY3N2KFwiQ1NWRGF0YVxcXFxnYWx0b25faGVpZ2h0LmNzdlwiLGhlYWRlciA9IFRSVUUpXG5oZWlnaHRzIDwtIHJlYWQuY3N2KFwiaHR0cHM6Ly9hc3NldHMuZGF0YWNhbXAuY29tL3Byb2R1Y3Rpb24vcmVwb3NpdG9yaWVzLzI2MTAvZGF0YXNldHMvYzg1ZWRlNmMyMDVkMjIwNDllNzY2YmQwODk1NmIyMjVjNTc2MjU1Yi9nYWx0b25faGVpZ2h0LmNzdlwiLCBoZWFkZXIgPSBUUlVFKVxuaHRfY2hpbGQgPC0gaGVpZ2h0cyRjaGlsZF9odFxubWNoaWxkIDwtIG1lYW4oaHRfY2hpbGQpXG5zZGNoaWxkIDwtIHNkKGh0X2NoaWxkKSIsInNhbXBsZSI6IiNWaXN1YWxpemUgdGhlIERpc3RyaWJ1dGlvblxuX19fKF9fXywgZnJlcSA9IEZBTFNFKVxuXG4jRGV0ZXJtaW5lIGEgc2VxdWVuY2UuIFRoZW4sIGdyYXBoIGEgaGlzdG9ncmFtIHdpdGggYSBub3JtYWwgY3VydmUgc3VwZXJpbXBvc2VkXG54IDwtIHNlcSg2MCwgODAsYnkgPSAwLjEpXG5fX18oeCwgZG5vcm0oeCxtZWFuID0gbWNoaWxkLCBzZCA9IHNkY2hpbGQpLCBjb2wgPSBcImJsdWVcIilcblxuIyBEZXRlcm1pbmUgdGhlIHByb2JhYmlsaXR5IHRoYXQgYSBjaGlsZCdzIGhlaWdodCBpcyBncmVhdGVyIHRoYW4gNzJcbnByb2IgPC0gMSAtIFxucHJvYiIsInNvbHV0aW9uIjoiaGlzdChodF9jaGlsZCwgZnJlcSA9IEZBTFNFKVxueCA8LSBzZXEoNjAsIDgwLGJ5ID0gMC4xKVxubGluZXMoeCwgZG5vcm0oeCwgbWVhbiA9IG1jaGlsZCwgc2QgPSBzZGNoaWxkKSwgY29sID0gXCJibHVlXCIpXG5wcm9iIDwtIDEgLSBwbm9ybSg3MiwgbWVhbiA9IG1jaGlsZCAsIHNkID0gc2RjaGlsZClcbnByb2IiLCJzY3QiOiJzdWNjZXNzX21zZyhcIkV4Y2VsbGVudCEgVmlzdWFsaXppbmcgYSBkaXN0cmlidXRpb24sIGVzcGVjaWFsbHkgd2l0aCByZWZlcmVuY2UgdG8gYSBub3JtYWwsIGlzIGltcG9ydGFudCBmb3IgY29tbXVuaWNhdGluZyByZXN1bHRzIG9mIHlvdXIgYW5hbHlzaXMuXCIpIn0= 1.2 Visualizing distributions In this section, you learn how to: Calculate and interpret distributions using histograms Calculate and interpret distributions using density plots 1.2.1 Video Video Overhead Details Show Overhead Details. Data description For our first look at an insurance data set, we consider data from Rempala and Derrig (2005). They considered claims arising from automobile bodily injury insurance coverages. These are amounts incurred for outpatient medical treatments that arise from automobile accidents, typically sprains, broken collarbones and the like. The data consists of a sample of 272 claims from Massachusetts that were closed in 2001 (by “closed,” we mean that the claim is settled and no additional liabilities can arise from the same accident). Rempala and Derrig were interested in developing procedures for handling mixtures of “typical” claims and others from providers who reported claims fraudulently. For this sample, we consider only those typical claims, ignoring the potentially fraudulent ones. # Reformat Data Set injury &lt;- read.csv(&quot;CSVData\\\\MassBodilyInjury.csv&quot;,header = TRUE) str(injury) head(injury) # PICK THE SUBSET OF THE DATA CORRESPONDING TO PROVIDER A injury2 &lt;- subset(injury, providerA ! = 0 ) injury2$claims &lt;- 1000*injury2$claims injury2$logclaims &lt;- log(injury2$claims) injury3 &lt;- injury2[c(&quot;claims&quot;,&quot;logclaims&quot;)] #write.csv(injury3,&quot;CSVData\\\\MassBI.csv&quot;,row.names = FALSE) Show Overhead A Details. Bring in Data, Introduce Logarithmic Claims injury &lt;- read.csv(&quot;CSVData\\\\MassBI.csv&quot;,header = TRUE) # CHECK THE NAMES, DIMENSION IN THE FILE AND LIST THE FIRST 8 OBSERVATIONS ; str(injury) head(injury) attach(injury) The following objects are masked from injury (pos = 3): claims, logclaims The following objects are masked from injury (pos = 4): claims, logclaims The following objects are masked from injury (pos = 5): claims, logclaims The following objects are masked from injury (pos = 6): claims, logclaims claims &lt;- injury$claims par(mfrow = c(1, 2)) hist(claims) hist(logclaims) &#39;data.frame&#39;: 272 obs. of 2 variables: $ claims : int 45 47 70 75 77 92 117 117 140 145 ... $ logclaims: num 3.81 3.85 4.25 4.32 4.34 ... claims logclaims 1 45 3.806662 2 47 3.850148 3 70 4.248495 4 75 4.317488 5 77 4.343805 6 92 4.521789 Show Overhead B Details. Show how to get a finer grid for histograms par(mfrow = c(1, 2)) hist(logclaims) hist(logclaims,breaks = 15) Show Overhead C Details. Introduce the density plot par(mfrow = c(1, 2)) plot(density(logclaims)) hist(logclaims, breaks = 15,freq = FALSE) lines(density(logclaims)) 1.2.2 Exercise. Visualizing bodily injury claims with density plots Assignment Text In the prior video, you learned about the Massachusetts bodily injury dataset. This dataframe, injury, has been read in and the global variable claims has been created. This assignment reviews the hist() function for visualizing distributions and allows you to explore density plotting, a smoothed version of the histogram. Instructions Use the function log() to create the logarithmic version of the claims variable Calculate a histogram of logarithmic with 40 bins using an option in the hist() function, breaks =. Create a density plot of logarithmic claims using the functions plot() and density(). Repeat the density plot, this time using a more refined bandwidth equal to 0.03. Use an option in the density() function, bw =. eyJsYW5ndWFnZSI6InIiLCJwcmVfZXhlcmNpc2VfY29kZSI6IiNpbmp1cnkgPC0gcmVhZC5jc3YoXCJDU1ZEYXRhXFxcXE1hc3NCSS5jc3ZcIixoZWFkZXIgPSBUUlVFKVxuaW5qdXJ5IDwtIHJlYWQuY3N2KFwiaHR0cHM6Ly9hc3NldHMuZGF0YWNhbXAuY29tL3Byb2R1Y3Rpb24vcmVwb3NpdG9yaWVzLzI2MTAvZGF0YXNldHMvOGNjYTE5ZDA1MDNmY2Y2ZTlkMzBkOWNiOTEyZGU1YmE5NWVjYjljMS9NYXNzQkkuY3N2XCIsIGhlYWRlciA9IFRSVUUpXG5jbGFpbXMgPC0gaW5qdXJ5JGNsYWltcyIsInNhbXBsZSI6IiNDcmVhdGUgdGhlIGxvZ2FyaXRobWljIGNsYWltcyB2YXJpYWJsZVxubG9nY2xhaW1zIDwtIF9fX1xuXG4jQ3JlYXRlIGEgaGlzdG9ncmFtIHVzaW5zIDQwIGJpbnNcbl9fXyhsb2djbGFpbXMsIGJyZWFrcyA9IDQwLGZyZXEgPSBGQUxTRSlcbmJveCgpXG5cbiMgQ3JlYXRlIGEgZGVuc2l0eSBwbG90IG9mIGxvZ2FyaXRobWljIGNsYWltc1xucGxvdChfX18obG9nY2xhaW1zKSlcblxuIyBDcmVhdGUgYSBkZW5zaXR5IHBsb3Qgb2YgbG9nYXJpdGhtaWMgY2xhaW1zIHdpdGggYSBzbWFsbGVyIGJhbmR3aWR0aFxuX19fIiwic29sdXRpb24iOiJsb2djbGFpbXMgPC0gbG9nKGNsYWltcylcbmhpc3QobG9nY2xhaW1zICwgYnJlYWtzID0gNDAsZnJlcSA9IEZBTFNFKVxuYm94KClcbnBsb3QoZGVuc2l0eShsb2djbGFpbXMpKVxucGxvdChkZW5zaXR5KGxvZ2NsYWltcywgYncgPSAwLjAzKSkiLCJzY3QiOiJzdWNjZXNzX21zZyhcIkV4Y2VsbGVudCEgVmlzdWFsaXppbmcgdGhlIGRpc3RyaWJ1dGlvbiBpcyBpbXBvcnRhbnQgYW5kIHNtb290aGluZyB0ZWNobmlxdWVzIGFsbG93IHZpZXdlcnMgdG8gc2VlIGltcG9ydGFudCBwYXR0ZXJucyB3aXRob3V0IGJlaW5nIGRpc3RyYWN0ZWQgYnkgcmFuZG9tIGZsdWN0YXRpb25zLlwiKSJ9 1.3 Summarizing distributions In this section, you learn how to: Calculate and interpret basic summary statistics Calculate and interpret distributions using boxplots Calculate and interpret distributions using qq plots 1.3.1 Video Video Overhead Details Show Overhead A Details. Summary statistics injury &lt;- read.csv(&quot;CSVData\\\\MassBI.csv&quot;,header = TRUE) #injury &lt;- read.csv(&quot;https://assets.datacamp.com/production/repositories/2610/datasets/8cca19d0503fcf6e9d30d9cb912de5ba95ecb9c1/MassBI.csv&quot;, header = TRUE) attach(injury) # SUMMARY STATISTICS summary(injury) sd(claims);sd(logclaims) length(claims) claims logclaims Min. : 45.0 Min. : 3.807 1st Qu.: 892.5 1st Qu.: 6.794 Median : 2210.0 Median : 7.701 Mean : 2697.7 Mean : 7.388 3rd Qu.: 3215.0 3rd Qu.: 8.076 Max. :50000.0 Max. :10.820 [1] 3944.445 [1] 1.10093 [1] 272 Show Overhead B Details. Boxplot # BASIC BOXPLOT boxplot(logclaims) quantile(logclaims, probs = 0.75) # BOXPLOT WITH ANNOTATION boxplot(logclaims, main = &quot;Boxplot of logclaims&quot;) text(1, 7.6, &quot;median&quot;, cex = 0.7) text(1, 6.55, &quot;25th percentile&quot;, cex = 0.7) text(1, 7.95, &quot;75th percentile&quot;, cex = 0.7) arrows(1.05, 4.9, 1.05, 3.6, col = &quot;blue&quot;, code = 3, angle = 20, length = 0.1) text(1.1, 4.4, &quot;outliers&quot;, cex = 0.7) text(1.1, 10.9, &quot;outlier&quot;, cex = 0.7) 75% 8.075579 Show Overhead C Details. QQ Plot summary(injury) quantile(claims, probs = 0.75) quantile(logclaims, probs = 0.75) log(quantile(claims, probs = 0.75)) qnorm(p = 0.75, mean = mean(logclaims), sd = sd(logclaims)) (qnorm(p = 0.75, mean = mean(logclaims), sd = sd(logclaims)) -mean(logclaims)) / sd(logclaims) qnorm(p = 0.75, mean = 0, sd = 1) # QUANTILE - QUANTILE PLOT qqnorm(logclaims) qqline(logclaims) claims logclaims Min. : 45.0 Min. : 3.807 1st Qu.: 892.5 1st Qu.: 6.794 Median : 2210.0 Median : 7.701 Mean : 2697.7 Mean : 7.388 3rd Qu.: 3215.0 3rd Qu.: 8.076 Max. :50000.0 Max. :10.820 75% 3215 75% 8.075579 75% 8.075583 [1] 8.131056 [1] 0.6744898 [1] 0.6744898 1.3.2 Exercise. Summarizing bodily injury claims with box and qq plots Assignment Text The Massachusetts bodily injury data has already been read and used to create the global variable claims representing bodily injury claims. The previous video showed how to present the distribution of logarithmic claims which appeared to be approximately normally distributed. However, users are not really interested in log dollars but want to know about a unit of measurement that is more intuitive, such as dollars. So this assignment is based on claims, not the logarithmic version. You will use the functions boxplot() and qqnorm() to visualize the distribution through boxplots and quantile-quantile, or qq-, plots. But, because we are working with such a skewed distribution, do not be surprised that it is difficult to interpret these results readily. Instructions Produce a box plot for claims Determine the 25th empirical percentile for claims using the quantile() function. Determine the 25th percentile for claims based on a normal distribution using the qnorm() function. Produce a normal qq plot for claims using the function qqnorm(). The qqline() function is handy for producing a reference line. Hint. Note that qnorm() (one q) is for a normal quantile and qqnorm(). (two q’s!) is for the normal qq plot eyJsYW5ndWFnZSI6InIiLCJwcmVfZXhlcmNpc2VfY29kZSI6IiNpbmp1cnkgPC0gcmVhZC5jc3YoXCJDU1ZEYXRhXFxcXE1hc3NCSS5jc3ZcIiwgaGVhZGVyID0gVFJVRSlcbmluanVyeSA8LSByZWFkLmNzdihcImh0dHBzOi8vYXNzZXRzLmRhdGFjYW1wLmNvbS9wcm9kdWN0aW9uL3JlcG9zaXRvcmllcy8yNjEwL2RhdGFzZXRzLzhjY2ExOWQwNTAzZmNmNmU5ZDMwZDljYjkxMmRlNWJhOTVlY2I5YzEvTWFzc0JJLmNzdlwiLCBoZWFkZXIgPSBUUlVFKVxuY2xhaW1zIDwtIGluanVyeSRjbGFpbXMiLCJzYW1wbGUiOiIjUHJvZHVjZSBhIGJveCBwbG90IGZvciBjbGFpbXNcbl9fXyhjbGFpbXMpXG5cbiNEZXRlcm1pbmUgdGhlIDI1dGggZW1waXJpY2FsIHBlcmNlbnRpbGUgZm9yIGNsYWltc1xucTI1IDwtIF9fXyhjbGFpbXMsIHByb2JzID0gX19fKVxucTI1XG5cbiNEZXRlcm1pbmUgdGhlIDI1dGggcGVyY2VudGlsZSBmb3IgY2xhaW1zIGJhc2VkIG9uIGEgbm9ybWFsIGRpc3RyaWJ1dGlvblxucW4yNSA8LSBfX18ocCA9IF9fXywgbWVhbiA9IG1lYW4oY2xhaW1zKSwgc2QgPSBzZChjbGFpbXMpKVxucW4yNVxuXG4jUHJvZHVjZSBhIG5vcm1hbCBxcSBwbG90IGZvciBjbGFpbXNcbl9fXyhjbGFpbXMpXG5fX18oY2xhaW1zKSIsInNvbHV0aW9uIjoiIyBTb2x1dGlvblxuYm94cGxvdChjbGFpbXMpXG5xMjUgPC0gcXVhbnRpbGUoY2xhaW1zLCBwcm9icyA9IDAuMjUpXG5xMjVcbnFuMjUgPC0gcW5vcm0ocCA9IDAuMjUsIG1lYW4gPSBtZWFuKGNsYWltcyksIHNkID0gc2QoY2xhaW1zKSlcbnFuMjVcbnFxbm9ybShjbGFpbXMpXG5xcWxpbmUoY2xhaW1zKSAgICIsInNjdCI6InN1Y2Nlc3NfbXNnKFwiQ29uZ3JhdHVsYXRpb25zIG9uIGxlYXJuaW5nIGFib3V0IGJveCBhbmQgcXEgcGxvdHMuIEFsdGhvdWdoIHlvdSBhcmUgdW5saWtlbHkgdG8gc2hvdyB0aGVzZSBwbG90cyB0byBjb25zdW1lcnMgb2YgeW91ciBhbmFseXNpcywgeW91IHdpbGwgZmluZCB0aGVtIHVzZWZ1bCB0b29scyBmb3IgZXhwbG9yaW5nIG11bHRpdmFyaWF0ZSBhc3BlY3RzIG9mIGRhdGEuXCIpIn0= 1.3.3 Exercise. Effects on distributions of removing the largest claim Assignment Text The Massachusetts bodily injury dataframe injury has been read in; our focus is on the claims variable in that dataset. In the previous exercise, we learned that the Massachusetts bodily injury claims distribution was not even close to approximately normal (as evidenced by the box and qq- plots). Non-normality may be induced by skewness (that we will handle via transformations in the next section). But, seeming non-normality can also be induced by one or two very large observations (called an outlier later in the course). So, this exercise examines the effects on the distribution of removing the largest claims. Instructions Use the function tail() to examine the injury dataset and identify the largest claim Use the function subset() to create a subset omitting the largest claim Compare the summary statistics of the omitted claim distribution to the full distribution Compare the two distributions visually via histograms plotted next to another. par(mfrow = c(1, 2)) is used to organize the plots you create. Do not alter this code. Hint. For this data set, the [subset()] argument claims &lt; 25000 will keep all but the largest claim eyJsYW5ndWFnZSI6InIiLCJwcmVfZXhlcmNpc2VfY29kZSI6IiNpbmp1cnkgPC0gcmVhZC5jc3YoXCJDU1ZEYXRhXFxcXE1hc3NCSS5jc3ZcIiwgaGVhZGVyID0gVFJVRSlcbmluanVyeSA8LSByZWFkLmNzdihcImh0dHBzOi8vYXNzZXRzLmRhdGFjYW1wLmNvbS9wcm9kdWN0aW9uL3JlcG9zaXRvcmllcy8yNjEwL2RhdGFzZXRzLzhjY2ExOWQwNTAzZmNmNmU5ZDMwZDljYjkxMmRlNWJhOTVlY2I5YzEvTWFzc0JJLmNzdlwiLCBoZWFkZXIgPSBUUlVFKVxuY2xhaW1zIDwtIGluanVyeSRjbGFpbXMiLCJzYW1wbGUiOiIjIEV4YW1pbmUgdGhlIHRhaWwgb2YgdGhlIGBpbmp1cnlgIGRhdGFzZXRcbnRhaWwoX19fKVxuXG4jIENyZWF0ZSBhIHN1YnNldCBvbWl0dGluZyB0aGUgbGFyZ2VzdCBjbGFpbVxuaW5qdXJ5MiA8LSBzdWJzZXQoaW5qdXJ5LCBfX18pXG5cbiMgQ29tcGFyZSB0aGUgc3VtbWFyeSBzdGF0aXN0aWNzIG9mIHRoZSBvbWl0dGVkIGNsYWltIGRpc3RyaWJ1dGlvbiB0byB0aGUgZnVsbCBkaXN0cmlidXRpb25cbnN1bW1hcnkoX19fKVxuc3VtbWFyeShpbmp1cnkyKVxuXG4jIENvbXBhcmUgdGhlIHR3byBkaXN0cmlidXRpb25zIHZpc3VhbGx5IHZpYSBoaXN0b2dyYW1zIHBsb3R0ZWQgbmV4dCB0byBhbm90aGVyXG5wYXIobWZyb3cgPSBjKDEsIDIpKVxuaGlzdChfX18sIGZyZXEgPSBGQUxTRSwgIG1haW4gPSBcIkZ1bGwgRGF0YVwiKVxuaGlzdChfX18sIGZyZXEgPSBGQUxTRSwgIG1haW4gPSBcIkxhcmdlc3QgQ2xhaW0gT21pdHRlZFwiKSIsInNvbHV0aW9uIjoidGFpbChpbmp1cnkpXG5pbmp1cnkyIDwtIHN1YnNldChpbmp1cnksIGNsYWltcyA8IDI1MDAwIClcbnN1bW1hcnkoaW5qdXJ5KVxuc3VtbWFyeShpbmp1cnkyKVxuXG5wYXIobWZyb3cgPSBjKDEsIDIpKVxuaGlzdChjbGFpbXMsIGZyZXEgPSBGQUxTRSwgIG1haW4gPSBcIkZ1bGwgRGF0YVwiKVxuaGlzdChpbmp1cnkyJGNsYWltcywgZnJlcSA9IEZBTFNFLCAgbWFpbiA9IFwiTGFyZ2VzdCBDbGFpbSBPbWl0dGVkXCIpIiwic2N0Ijoic3VjY2Vzc19tc2coXCJDb25ncmF0dWxhdGlvbnMhIFRoZSBnb2FsIG9mIHByZWRpY3RpdmUgbW9kZWxpbmcgaXMgdG8gZGlzY292ZXIgcGF0dGVybnMgaW4gdGhlIGRhdGEuIEhvd2V2ZXIsIHNvbWV0aW1lcyBzZWVtaW5nICdwYXR0ZXJucycgYXJlIHRoZSByZXN1bHQgb2Ygb25lIG9yIHR3byB1bnVzdWFsIG9ic2VydmF0aW9ucy4gVW51c3VhbCBvYnNlcnZhdGlvbnMgbWF5IGJlIGR1ZSB0byBpbmNvcnJlY3QgZGF0YSBnYXRoZXJpbmcgcHJvY2VkdXJlcyBvciBqdXN0IGR1ZSB0byB3aWxkIGZsdWN0dWF0aW9ucyBpbiBhIHByb2Nlc3Mgb2YgaW50ZXJlc3QgYnV0IGFyZSBjb21tb24gaW4gcHJlZGljdGl2ZSBtb2RlbGluZy5cIikifQ== 1.4 Transformations In this exercise, you learn how to: Symmetrize a skewed distribution using a logarithmic transformation 1.4.1 Video Video Overhead Details Show Overhead A Details. Simulate a moderately skewed distribution, with transforms # FIGURE 1.7 - SIMULATE CHI-SQUARE, CREATE 3 TRANSFORMATIONS set.seed(1237) # set the seed of the random number generator # allows us to replicate results X1 &lt;- 10000*rchisq(500, df = 2) # generate variables randomly from a skewed distribution X2 &lt;- X1^(0.5) # square root transform, could also use sqrt(X1) X3 &lt;- log(X1) # logarithmic transform X4 &lt;- -1/X1 # negative reciprocal transform Show Overhead B Details. Visualize the distributions par(mfrow = c(2, 2), cex = .75, mar = c(3,5,1.5,0)) hist(X1, freq = FALSE, nclass = 16, main = &quot;&quot;, xlab = &quot;&quot;, ylab = &quot;&quot;, las = 1, yaxt = &quot;n&quot;,xlim = c(0,200000),ylim = c(0,.00005)) axis(2, at = seq(0,.00005,.00001),las = 1, cex = .3, labels = c(&quot;0&quot;, &quot;0.00001&quot;, &quot;0.00002&quot;,&quot;0.00003&quot;, &quot;0.00004&quot;, &quot;0.00005&quot;)) mtext(&quot;Density&quot;, side = 2, at = .000055, las = 1, cex = .75) mtext(&quot;y&quot;, side = 1, cex = .75, line = 2) par(mar = c(3,4,1.5,0.2)) hist(X2, freq = FALSE, nclass = 16, main = &quot;&quot;, xlab = &quot;&quot;, ylab = &quot;&quot;, las = 1,xlim = c(0,400), ylim = c(0,.008)) mtext(&quot;Density&quot;, side = 2, at = .0088, las = 1, cex = .75) mtext(&quot;Square root of y&quot;, side = 1, cex = .75, line = 2) par(mar = c(3.2,5,1,0)) hist(X3, freq = FALSE, nclass = 16, main = &quot;&quot;, xlab = &quot;&quot;, ylab = &quot;&quot;, las = 1, ylim = c(0,.4)) mtext(&quot;Density&quot;, side = 2, at = .44, las = 1, cex = .75) mtext(&quot;Logarithmic y&quot;, side = 1, cex = .75, line = 2) par(mar = c(3.2,4,1,0.2)) hist(X4, freq = FALSE, nclass = 16, main = &quot;&quot;,xlab = &quot;&quot;, ylab = &quot;&quot;, las = 1, ylim = c(0,100)) mtext(&quot;Density&quot;, side = 2, at = 110, las = 1, cex = .75) mtext(&quot;Negative reciprocal of y&quot;, side = 1, cex = .75, line = 2) 1.4.2 Exercise. Distribution of transformed bodily injury claims Assignment Text We have now examined the distributions of bodily injury claims and its logarithmic version. Grudgingly, we have concluded that to fit a normal curve the logarithmic version of claims is a better choice (again, we really do not like log dollars but you’ll get used to it in this course). But, why logarithmic and not some other transformations? A partial response to this question will appear in later chapters when we describe interpretation of regression coefficients. Another partial response is that the log transform seems to work well with skewed insurance data sets, as we demonstrate visually in this exercise. Instructions Use the code par(mfrow = c(2, 2)) so that four graphs appear in a 2 by 2 matrix format for easy comparisons. Plot the density() of claims square root of claims logarithmic claims negative reciprocal of claims Hint. For negative reciprocal claims, use plot(density(-claims^(-1))) eyJsYW5ndWFnZSI6InIiLCJwcmVfZXhlcmNpc2VfY29kZSI6IiNpbmp1cnkgPC0gcmVhZC5jc3YoXCJDU1ZEYXRhXFxcXE1hc3NCSS5jc3ZcIiwgaGVhZGVyID0gVFJVRSlcbmluanVyeSA8LSByZWFkLmNzdihcImh0dHBzOi8vYXNzZXRzLmRhdGFjYW1wLmNvbS9wcm9kdWN0aW9uL3JlcG9zaXRvcmllcy8yNjEwL2RhdGFzZXRzLzhjY2ExOWQwNTAzZmNmNmU5ZDMwZDljYjkxMmRlNWJhOTVlY2I5YzEvTWFzc0JJLmNzdlwiLCBoZWFkZXIgPSBUUlVFKVxuY2xhaW1zIDwtIGluanVyeSRjbGFpbXMiLCJzYW1wbGUiOiIjVGhpcyBjb2RlIGhlbHBzIHRvIG9yZ2FuaXplIHRoZSBmb3VyIGdyYXBocyBpbnRvIGEgMiBieSAyIGZvcm1hdFxucGFyKG1mcm93ID0gYygyLCAyKSlcbiNQbG90IHRoZSBkZW5zaXR5IG9mIGNsYWltc1xucGxvdChkZW5zaXR5KF9fXykpXG5cbiNQbG90IHRoZSBkZW5zaXR5IG9mIHNxdWFyZSByb290IG9mIGNsYWltc1xucGxvdChkZW5zaXR5KF9fXykpIFxuXG4jUGxvdCB0aGUgZGVuc2l0eSBvZiBsb2dhcml0aG1pYyBjbGFpbXNcbnBsb3QoZGVuc2l0eShfX18pKVxuXG4jUGxvdCB0aGUgZGVuc2l0eSBvZiB0aGUgbmVnYXRpdmUgcmVjaXByb2NhbCBvZiBjbGFpbXNcbnBsb3QoZGVuc2l0eShfX18pKSIsInNvbHV0aW9uIjoicGFyKG1mcm93ID0gYygyLCAyKSlcbnBsb3QoZGVuc2l0eShjbGFpbXMpKSAgICBcbnBsb3QoZGVuc2l0eShjbGFpbXNeKDAuNSkpKSAgXG5wbG90KGRlbnNpdHkobG9nKGNsYWltcykpKSAgXG5wbG90KGRlbnNpdHkoLWNsYWltc14oLTEpKSkgICIsInNjdCI6InN1Y2Nlc3NfbXNnKFwiRXhjZWxsZW50ISBUcmFuc2Zvcm1hdGlvbnMgb2YgZGF0YSBpcyBhIHRvb2wgdGhhdCBpbmNyZWRpYmx5IGV4cGFuZHMgcG90ZW50aWFsIGFwcGxpY2FiaWxpdHkgb2YgKGxpbmVhcikgcmVncmVzc2lvbiB0ZWNobmlxdWVzLlwiKSJ9 "],
["basic-linear-regression.html", "Chapter 2 Basic Linear Regression 2.1 Correlation 2.2 Method of least squares 2.3 Understanding variability 2.4 Statistical inference 2.5 Diagnostics", " Chapter 2 Basic Linear Regression Chapter description This chapter considers regression in the case of only one explanatory variable. Despite this seeming simplicity, many deep ideas of regression can be developed in this framework. By limiting ourselves to the one variable case, we can illustrate the relationships between two variables graphically. Graphical tools prove to be important for developing a link between the data and a predictive model. 2.1 Correlation In this section, you learn how to: Calculate and interpret a correlation coefficient Interpret correlation coefficients by visualizing scatter plots 2.1.1 Video Video Overhead Details Show Overhead A Details. Wisconsin lottery data description Lot &lt;- read.csv(&quot;CSVData\\\\Wisc_lottery.csv&quot;) #Lot &lt;- read.csv(&quot;https://assets.datacamp.com/production/repositories/2610/datasets/a792b30fb32b0896dd6894501cbab32b5d48df51/Wisc_lottery.csv&quot;, header = TRUE) str(Lot) Show Overhead B Details. Summary statistics #options(scipen = 100, digits = 4) #numSummary(Lot[,c(&quot;pop&quot;, &quot;sales&quot;)], statistics = c(&quot;mean&quot;, &quot;sd&quot;, &quot;quantiles&quot;), quantiles = c(0,.5,1)) (as.data.frame(psych::describe(Lot)))[,c(2,3,4,5,8,9)] #Rcmdr::numSummary(Lot[,c(&quot;pop&quot;, &quot;sales&quot;)], statistics = c(&quot;mean&quot;, &quot;sd&quot;, &quot;quantiles&quot;), quantiles = c(0,.5,1)) Show Overhead C Details. Visualizing skewed distributions par(mfrow = c(1, 2)) hist(Lot$pop, main = &quot;&quot;, xlab = &quot;population&quot;) hist(Lot$sales, main = &quot;&quot;, xlab = &quot;sales&quot;) Show Overhead D Details. Visualizing relationships with a scatter plot plot(Lot$pop, Lot$sales, xlab = &quot;population&quot;, ylab = &quot;sales&quot;) Show Overhead E Details. Correlation coefficient cor(Lot$pop, Lot$sales) 2.1.2 Exercise. Correlations and the Wisconsin lottery Assignment Text The Wisconsin lottery dataset, Wisc_lottery,has already been read into a dataframe Lot. Like insurance, lotteries are uncertain events and so the skills to work with and interpret lottery data are readily applicable to insurance. It is common to report sales and population in thousands of units, so this exercise gives you practice in rescaling data via linear transformations. Instructions From the available population and sales variables, create new variables in the dataframe Lot, pop_1000 and sales_1000 that are in thousands (of people and of dollars, respectively). Create summary statistics for the dataframe that includes these new variables. Plot pop_1000 versus sales_1000. Calculate the correlation between pop_1000 versus sales_1000 using the function cor(). How does this differ between the correlation between population and sales in the original units? Hint Use the dataframe to refer to pop and sales as Lot$pop and Lot$sales, respectively Pre-exercise code # Pre-exercise code #library(Rcmdr) #library(psych) Lot &lt;- read.csv(&quot;CSVData\\\\Wisc_lottery.csv&quot;, header = TRUE) #Lot &lt;- read.csv(&quot;https://assets.datacamp.com/production/repositories/2610/datasets/a792b30fb32b0896dd6894501cbab32b5d48df51/Wisc_lottery.csv&quot;, header = TRUE) Sample_code `@Sample_code` # Create new variables, say, `pop_1000` and `sales_1000` Lot$pop_1000 &lt;- ___ ___ &lt;- Lot$sales/1000 # Create summary statistics for the dataframe summary(___) # Plot `pop_1000` versus `sales_1000`. plot(___, ___) # Calculate the correlation between `pop_1000` versus `sales_1000` cor(___, ___) Solution # Solution Lot$pop_1000 &lt;- Lot$pop/1000 Lot$sales_1000 &lt;- Lot$sales/1000 summary(Lot) #(as.data.frame(psych::describe(Lot)))[,c(2,3,4,5,8,9)] #(as.data.frame(psych::describe(Lot[,c(&quot;pop_1000&quot;, &quot;sales_1000&quot;)])))[,c(2,3,4,5,8,9)] #Rcmdr::numSummary(Lot[,c(&quot;pop_1000&quot;, &quot;sales_1000&quot;)], statistics = c(&quot;mean&quot;, &quot;sd&quot;, &quot;quantiles&quot;), quantiles = c(0,.5,1)) plot(Lot$pop_1000, Lot$sales_1000) cor(Lot$pop_1000, Lot$sales_1000) Submission Correctness Tests (SCT) test_error() test_object(“Lot”, incorrect_msg = “Looks like a variable is defined incorrectly. The hint may help.”) success_msg(“Congratulations! We will rescale data using ‘linear’ transformations regularly. In part we do this for communicating our analysis to others. Also in part, this is for our own convenience as it can allow us to see patterns more readily.”) 2.2 Method of least squares In this section, you learn how to: Fit a line to data using the method of least squares Predict an observation using a least squares fitted line 2.2.1 Video 2.2.1.1 Video Overheads Show Overhead A Details. Where to fit the line? model_blr &lt;- lm(sales ~ pop, data = Lot) plot(Lot$pop, Lot$sales,xlab = &quot;population&quot;, ylab = &quot;sales&quot;) abline(model_blr, col=&quot;blue&quot;) abline(0,1, col=&quot;red&quot;) Show Overhead B Details. Method of least squares For observation \\(\\{(y, x)\\}\\), the height of the regression line is \\[b_0 + b_1 x.\\] Thus, \\(y - (b_0 + b_1 x)\\) represents the deviation. The sum of squared deviations is \\[SS(b_0, b_1) = \\sum (y - (b_0 + b_1 x))^2 .\\] The method of least squares – determine values of \\(b_0, b_1\\) that minimize \\(SS\\). Show Overhead C Details. Regression coefficients model_blr &lt;- lm(sales ~ pop, data = Lot) round(coefficients(model_blr), digits=4) plot(Lot$pop, Lot$sales,xlab = &quot;population&quot;, ylab = &quot;sales&quot;) abline(model_blr, col=&quot;blue&quot;) Show Overhead D Details. Prediction round(coefficients(model_blr), digits=6) coefficients(model_blr)[1] + coefficients(model_blr)[2]*30000 newdata &lt;- data.frame(pop = 30000) predict(model_blr, newdata) 2.2.2 Exercise. Least squares fit using housing prices Assignment Text The prior video analyzed the effect that a zip code’s population has on lottery sales. Instead of population, suppose that you wish to understand the effect that housing prices have on the sale of lottery tickets. The dataframe Lot, read in from the Wisconsin lottery dataset Wisc_lottery, contains the variable medhome which is the median house price for each zip code, in thousands of dollars. In this exercise, you will get a feel for the distribution of this variable by examining summary statistics, examine its relationship with sales graphically and via correlations, fit a basic linear regression model and use this model to predict sales. Instructions Summarize the dataframe Lot that contains medhome and sales. Plot medhome versus sales. Summarize this relationship by calculating the corresponding correlation coefficient using the function cor(). Using the function lm(), regress medhome, the explanatory variable, on sales, the outcome variable. Display the regression coefficients to four significant digits. Use the function predict() and the fitted regression model to predict sales assuming that the median house price for a zip code is 50 (in thousands of dollars). Hint Pre-exercise code # Pre-exercise code #library(Rcmdr) Lot &lt;- read.csv(&quot;CSVData\\\\Wisc_lottery.csv&quot;, header = TRUE) #Lot &lt;- read.csv(&quot;https://assets.datacamp.com/production/repositories/2610/datasets/a792b30fb32b0896dd6894501cbab32b5d48df51/Wisc_lottery.csv&quot;, header = TRUE) Sample_code `@Sample_code` # Summarize the dataframe `Lot` that contains `medhome` and `sales` summary(Lot) # Plot and calculate the correlation of `medhome` versus `sales`. cor(___, ___) plot(___, ___) # Regress `medhome` on `sales`. Display the regression coefficients to four significant digits. model_blr1 &lt;- lm(___ ~ ___, data = Lot) round(coefficients(model_blr1), digits= ---) # Predict sales assuming that the median house price is 50 newdata &lt;- data.frame(medhome = ___) predict(model_blr1, newdata) Solution # Solution #numSummary(Lot[,c(&quot;medhome&quot;, &quot;sales&quot;)], statistics = c(&quot;mean&quot;, &quot;sd&quot;, &quot;quantiles&quot;), quantiles = c(0,.5,1)) #(as.data.frame(psych::describe(Lot[,c(&quot;medhome&quot;, &quot;sales&quot;)])))[,c(2,3,4,5,8,9)] summary(Lot) cor(Lot$medhome,Lot$sales) plot(Lot$medhome,Lot$sales) model_blr1 &lt;- lm(sales ~ medhome, data = Lot) round(coefficients(model_blr1), digits=4) newdata &lt;- data.frame(medhome = 50) predict(model_blr1, newdata) Submission Correctness Tests (SCT) test_error() test_object(“model_blr1”, incorrect_msg = “The basic linear regression model is incorrectly specified.”) test_object(“newdata”, incorrect_msg = “The new data is incorrectly specified.”) success_msg(“Congratulations! You now have experience fitting a regression line and using this line for predictions, just as Galton did when he used parents’ heights to predict the height of an adult child. Well done!”) 2.3 Understanding variability In this section, you learn how to: Visualize the ANOVA decomposition of variability Calculate and interpret \\(R^2\\), the coefficient of determination Calculate and interpret \\(s^2\\) the mean square error Explain the components of the ANOVA table 2.3.1 Video Video Overhead Details Show OverheadS A and B Details. Visualizing the uncertainty about a line par(mar=c(2.2,2.1,.2,.2),cex=1.2) x &lt;- seq(-4, 4, len=101) y &lt;- x plot(x, y, type = &quot;l&quot;, xlim=c(-3, 4), xaxt=&quot;n&quot;, yaxt=&quot;n&quot;, xlab=&quot;&quot;, ylab=&quot;&quot;) axis(1, at = c(-1, 1),lab = expression(bar(x), x)) axis(2, at = c(-1, 1, 3),lab = expression(bar(y), hat(y), y), las=1) abline(-1, 0, lty = 2) segments(-4, 1, 1, 1, lty=2) segments(-4, 3, 1, 3, lty = 2) segments(1, -4, 1, 3, lty = 2) segments(-1, -4, -1, -1, lty = 2) points(1, 3, cex=1.5, pch=19) arrows(1.0, 1, 1.0, 3, code = 3, lty = 1, angle=15, length=0.12, lwd=2) text(1.3, 2.2, expression( y-hat(y)),cex=0.8) text(-.3,2.2,&quot;&#39;unexplained&#39; deviation&quot;, cex=.8) arrows(1.0, -1, 1.0, 1, code = 3, lty = 1, angle=15, length=0.12, lwd=2) text(1.85, 0, expression(hat(y)-bar(y) == b[1](x-bar(x)) ), cex=0.8 ) text(2.1, -0.5, &quot; &#39;explained&#39; deviation&quot;, cex=0.8) arrows(-1, -1.0, 1, -1.0, code = 3, lty = 1, angle=15, length=0.12, lwd = 2) text(0, -1.3, expression( x-bar(x)), cex=0.8 ) text(3.5, 2.7, expression( hat(y)== b[0]+ b[1]*x), cex=0.8 ) Show Overheads C, D and E Details. ANOVA Table model_blr &lt;- lm(sales ~ pop, data = Lot) anova(model_blr) sqrt(anova(model_blr)$Mean[2]) summary(model_blr)$r.squared 2.3.2 Exercise. Summarizing measures of uncertainty Assignment Text In a previous exercise, you developed a regression line to fit the variable medhome, the median house price for each zip code, as a predictor of lottery sales. The regression of medhome on sales has been summarized in the R object model_blr. How reliable is the regression line? In this excercise, you will compute some of the standard measures that are used to summarize the goodness of this fit. Instructions Summarize the fitted regression model in an ANOVA table. Determine the size of the typical residual, \\(s\\). Determine the coefficient of determination, \\(R^2\\). Hint Learn more about possibilities through the Rdocumentation site. If you have not done so already, check out the function anova() Pre-exercise code # Pre-exercise code Lot &lt;- read.csv(&quot;CSVData\\\\Wisc_lottery.csv&quot;, header = TRUE) #Lot &lt;- read.csv(&quot;https://assets.datacamp.com/production/repositories/2610/datasets/a792b30fb32b0896dd6894501cbab32b5d48df51/Wisc_lottery.csv&quot;, header = TRUE) Sample_code `@Sample_code` model_blr &lt;- lm(sales ~ medhome, data = Lot) # Summarize the fitted regression model in an ANOVA table. anova(___) # Determine the size of the typical residual, $s$. sqrt(anova(___)$Mean[2]) # Determine the coefficient of determination, $R^2$. summary(___)$r.squared Solution # Solution model_blr &lt;- lm(sales ~ medhome, data = Lot) anova(model_blr) sqrt(anova(model_blr)$Mean[2]) summary(model_blr)$r.squared Submission Correctness Tests (SCT) test_error() test_object(“model_blr”, incorrect_msg = “The basic linear regression model is incorrectly specified.”) success_msg(“Congratulations! It will be helpful if you compare the results of this exercise to the regression of pop on sales from the prior video. We have seen that pop is more highly correlated with sales than medhome, so we are expecting greater uncertainty in this regression fit.”) 2.3.3 Exercise. Effects of linear transforms on measures of uncertainty Assignment Text Let us see how rescaling, a linear transformation, affects our measures of uncertainty. As before, the Wisconsin lottery dataset Wisc_lottery has been read into a dataframe Lot that also contains sales_1000, sales in thousands of dollars, and pop_1000, zip code population in thousands. How do measures of uncertainty change when going from the original units to thousands of those units? Instructions Run a regression of pop on sales_1000 and summarize this in an ANOVA table. For this regression, determine the \\(s\\) and the coefficient of determination, \\(R^2\\). Run a regression of pop_1000 on sales_1000 and summarize this in an ANOVA table. For this regression, determine the \\(s\\) and the coefficient of determination, \\(R^2\\). Hint The residual standard error is also available as summary(model_blr1)$sigma. The coefficient of determination is also available as summary(model_blr1)$r.squared. Pre-exercise code # Pre-exercise code #library(Rcmdr) Lot &lt;- read.csv(&quot;CSVData\\\\Wisc_lottery.csv&quot;, header = TRUE) #Lot &lt;- read.csv(&quot;https://assets.datacamp.com/production/repositories/2610/datasets/a792b30fb32b0896dd6894501cbab32b5d48df51/Wisc_lottery.csv&quot;, header = TRUE) Lot$pop_1000 &lt;- Lot$pop/1000 Lot$sales_1000 &lt;- Lot$sales/1000 Sample_code `@Sample_code` # Run a regression of `pop` on `sales_1000` and summarize this in an ANOVA table. model_blr1 &lt;- lm(sales_1000 ~ pop, data = Lot) anova(___) # Determine the $s$ and the coefficient of determination, $R^2$. sqrt(anova(___)$Mean[2]) summary(___)$r.squared # Run a regression of `pop_1000` on `sales_1000` and summarize this in an ANOVA table. model_blr2 &lt;- lm(___ ~ ___, data = Lot) anova(model_blr2) # Determine the $s$ and the coefficient of determination, $R^2$. ___ ___ Solution # Solution model_blr1 &lt;- lm(sales_1000 ~ pop, data = Lot) anova(model_blr1) sqrt(anova(model_blr1)$Mean[2]) summary(model_blr1)$r.squared model_blr2 &lt;- lm(sales_1000 ~ pop_1000 , data = Lot) anova(model_blr2) sqrt(anova(model_blr2)$Mean[2]) summary(model_blr2)$r.squared Submission Correctness Tests (SCT) test_error() test_object(“model_blr1”, incorrect_msg = “The basic linear regression model is incorrectly specified.”) success_msg(“Congratulations! In this exercise, you have seen that rescaling does not affect our measures of goodness of fit in any meaningful way. For example, the coefficient of determinations are completely unaffected. This is helpful because we will rescale variables extensively in our search for patterns in the data.”) 2.4 Statistical inference In this section, you learn how to: Conduct a hypothesis test for a regression coefficient using either a rejection/acceptance procedure or a p-value Calculate and interpret a confidence interval for a regression coefficient Calculate and interpret a prediction interval at a specific value of a predictor variable 2.4.1 Video Video Overhead Details Show Overhead A Details. Summary of basic linear regression model Introduce the output in the summary of the basic linear regression model. Lot &lt;- read.csv(&quot;CSVData\\\\Wisc_lottery.csv&quot;, header = TRUE) #Lot &lt;- read.csv(&quot;https://assets.datacamp.com/production/repositories/2610/datasets/a792b30fb32b0896dd6894501cbab32b5d48df51/Wisc_lottery.csv&quot;, header = TRUE) #options(scipen = 8, digits = 4) model_blr &lt;- lm(sales ~ pop, data = Lot) summary(model_blr) Show Overhead B Details. Hypothesis testing &gt; summary(model_blr)$coefficients Estimate Std. Error t value Pr(&gt;|t|) (Intercept) 469.7036 702.90619 0.6682 5.072e-01 pop 0.6471 0.04881 13.2579 1.158e-17 Show Overhead C Details. Confidence intervals Rcmdr::Confint(model_blr, level = .90) Rcmdr::Confint(model_blr, level = .95) Show Overhead D Details. Confidence intervals check # Just for checking summary(model_blr)$coefficients[2,1] summary(model_blr)$coefficients[2,2] qt(.975, 48) summary(model_blr)$coefficients[2,1] - summary(model_blr)$coefficients[2,2]*qt(.975, 48) Rcmdr::Confint(model_blr, level = .95) confint(model_blr, level = .95) Show Overhead E Details. Prediction intervals NewData &lt;- data.frame(pop = 10000) predict(model_blr, NewData, interval = &quot;prediction&quot;, level = .90) predict(model_blr, NewData, interval = &quot;prediction&quot;, level = .99) 2.4.2 Exercise. Statistical inference and Wisconsin lottery Assignment Text In a previous exercise, you developed a regression line with the variable medhome, the median house price for each zip code, as a predictor of lottery sales. The regression of medhome on sales has been summarized in the R object model_blr. This exercise allows you to practice the standard inferential tasks: hypothesis testing, confidence intervals, and prediction. Instructions Summarize the regression model and identify the t-statistic for testing the importance of the regression coefficient associated with medhome. Use the function confint() to provide a 95% confidence interval for the regression coefficient associated with medhome. Consider a zip code with a median housing price equal to 50 (in thousands of dollars). Use the function predict() to provide a point prediction and a 95% prediction interval for sales. Hint Taking a [summary()] of a regression object produces a new objeect. You can use the [str()] structure command to learn more about the new object. Try out a command such as str(summary(model_blr)) Pre-exercise code # Pre-exercise code Lot &lt;- read.csv(&quot;CSVData\\\\Wisc_lottery.csv&quot;, header = TRUE) #Lot &lt;- read.csv(&quot;https://assets.datacamp.com/production/repositories/2610/datasets/a792b30fb32b0896dd6894501cbab32b5d48df51/Wisc_lottery.csv&quot;, header = TRUE) Sample_code `@sample_code` model_blr1 &lt;- lm(sales ~ medhome, data = Lot) # Summarize the regression model and identify the $t$-statistic for testing the importance of the regression coefficient associated with `medhome`. summary(___) summary(___)$coefficients summary(___)$coefficients[,3] # Provide a 95\\% confidence interval for the regression coefficient associated with `medhome`. confint(___, level = ___) # Provide a point prediction and a 95\\% prediction interval for sales. NewData1 &lt;- data.frame(medhome = 50) predict(___, NewData1, interval = &quot;prediction&quot;, level = ___) Solution # Solution model_blr1 &lt;- lm(sales ~ medhome, data = Lot) summary(model_blr1) summary(model_blr1)$coefficients summary(model_blr1)$coefficients[,3] #Rcmdr::Confint(model_blr1, level = .95) confint(model_blr1, level = .95) NewData1 &lt;- data.frame(medhome = 50) predict(model_blr1, NewData1, interval = &quot;prediction&quot;, level = .95) Submission Correctness Tests (SCT) test_error() test_object(“model_blr1”, incorrect_msg = “The basic linear regression model is incorrectly specified.”) test_object(“NewData1”, incorrect_msg = “The new data object is incorrectly specified.”) success_msg(“Congratulations! Much of what we learn from a data modeling exercise can be summarized using standard inferential tools: hypothesis testing, confidence intervals, and prediction.”) 2.5 Diagnostics In this section, you learn how to: Describe how diagnostic checking and residual analysis are used in a statistical analysis Describe several model misspecifications commonly encountered in a regression analysis 2.5.1 Video Video Overhead Details Show Overhead A Details. Unusual observations in regression We have defined regression estimates as minimizers of a least squares objective function. An appealing intuitive feature of linear regressions is that regression estimates can be expressed as weighted averages of outcomes. The weights vary by observation, some observations are more important than others. “Unusual” observations are far from the majority of the data set: Unusual in the vertical direction is called an outlier. Unusual in the horizontal directional is called a high leverage point. Show Overhead B Details. Example. Outliers and High Leverage Points outlr &lt;- read.csv(&quot;CSVData\\\\Outlier.csv&quot;, header = TRUE) # FIGURE 2.7 plot(outlr$x, outlr$y, xlim = c(0, 10), ylim = c(2, 9), xlab = &quot;x&quot;, ylab = &quot;y&quot;) text(4.5, 8.0, &quot;A&quot;) text(9.8, 8.0, &quot;B&quot;) text(9.8, 2.5, &quot;C&quot;) Show Overhead C Details. Regression fit with 19 base observations model_outlr0 &lt;- lm(y ~ x, data = outlr, subset = -c(20,21,22)) summary(model_outlr0) plot(outlr$x[1:19], outlr$y[1:19], xlab = &quot;x&quot;, ylab = &quot;y&quot;, xlim = c(0, 10), ylim = c(2, 9)) abline(model_outlr0) Show Overhead D Details. Regression fit with 19 base observations plus C model_outlrC &lt;- lm(y ~ x, data = outlr, subset = -c(20,21)) summary(model_outlrC) plot(outlr$x[c(1:19,22)], outlr$y[c(1:19,22)], xlab = &quot;x&quot;, ylab = &quot;y&quot;, xlim = c(0, 10), ylim = c(2, 9)) text(9.8, 2.5, &quot;C&quot;, col = &quot;blue&quot;) abline(model_outlrC) Show Overhead E Details. R code model_outlr0 &lt;- lm(y ~ x, data = outlr, subset = -c(20,21,22)) summary(model_outlr0) model_outlrA &lt;- lm(y ~ x, data = outlr, subset = -c(21,22)) summary(model_outlrA) model_outlrB &lt;- lm(y ~ x, data = outlr, subset = -c(20,22)) summary(model_outlrB) model_outlrC &lt;- lm(y ~ x, data = outlr, subset = -c(20,21)) summary(model_outlrC) Show Overhead F Details. Visualizing four regression fits plot(outlr$x, outlr$y, xlim = c(0, 10), ylim = c(2, 9), xlab = &quot;x&quot;, ylab = &quot;y&quot;) text(4.5, 8.0, &quot;A&quot;, col = &quot;red&quot;) text(9.8, 8.0, &quot;B&quot;, col = &quot;green&quot;) text(9.8, 2.5, &quot;C&quot;, col = &quot;blue&quot;) abline(model_outlr0) abline(model_outlrA, col = &quot;red&quot;) abline(model_outlrB, col = &quot;green&quot;) abline(model_outlrC, col = &quot;blue&quot;) Show Overhead G Details. Results from four regression models \\[\\begin{matrix} \\begin{array}{c} \\text{Results from Four Regressions} \\end{array}\\\\\\scriptsize \\begin{array}{l|rrrrr} \\hline \\text{Data} &amp; b_0 &amp; b_1 &amp; s &amp; R^2(\\%) &amp; t(b_1) \\\\ \\hline \\text{19 Base Points} &amp; 1.869 &amp; 0.611 &amp; 0.288 &amp; 89.0 &amp; 11.71 \\\\ \\text{19 Base Points} ~+~ A &amp; 1.750 &amp; 0.693 &amp; 0.846 &amp; 53.7 &amp; 4.57 \\\\ \\text{19 Base Points} ~+~ B &amp; 1.775 &amp; 0.640 &amp; 0.285 &amp; 94.7 &amp; 18.01 \\\\ \\text{19 Base Points} ~+~ C &amp; 3.356 &amp; 0.155 &amp; 0.865 &amp; 10.3 &amp; 1.44 \\\\ \\hline \\end{array} \\end{matrix}\\] 2.5.2 Exercise. Assessing outliers in lottery sales Assignment Text In an earlier video, we made a scatter plot of population versus sales. This plot exhibits an outlier; the point in the upper left-hand side of the plot represents a zip code that includes Kenosha, Wisconsin. Sales for this zip code are unusually high given its population. This exercise summarizes the regression fit both with and without this zip code in order to see how robust our results are to the inclusion of this unusual observation. Instructions A basic linear regression fit of population on sales has already been fit in the object model_blr. Re-fit this same model to the data, this time omitting Kenosha (observation number 9). Plot these two least squares fitted lines superimposed on the full data set. What is the effect on the distribution of residuals by removing this point? Calculate a normal qq plot with and without Kenosha. Hint You can extract the residuals from a regression object with the function [residuals()]. Pre-exercise code # Pre-exercise code Lot &lt;- read.csv(&quot;CSVData\\\\Wisc_lottery.csv&quot;, header = TRUE) #Lot &lt;- read.csv(&quot;https://assets.datacamp.com/production/repositories/2610/datasets/a792b30fb32b0896dd6894501cbab32b5d48df51/Wisc_lottery.csv&quot;, header = TRUE) Sample_code `@sample_code` model_blr &lt;-lm(sales ~ pop, data = Lot) summary(model_blr) # Re-fit this model to the data, this time omitting Kenosha (observation number 9). model_Kenosha &lt;- lm(___ ~ ___, data = Lot, subset = -c(9)) summary(___) # Plot these two least squares fitted lines superimposed on the full data set. plot(___, ___, xlab = &quot;population&quot;, ylab = &quot;sales&quot;) text(5000, 24000, &quot;Kenosha&quot;) abline(model_blr, col=&quot;blue&quot;) abline(___, col=&quot;red&quot;) # Calculate a normal qq plot with and without Kenosha. par(mfrow = c(1, 2)) qqnorm(residuals(___), main = &quot;&quot;) qqline(residuals(___))) qqnorm(residuals(___)), main = &quot;&quot;) qqline(residuals(___))) Solution # Solution model_blr &lt;-lm(sales ~ pop, data = Lot) summary(model_blr) model_Kenosha &lt;- lm(sales ~ pop, data = Lot, subset = -c(9)) summary(model_Kenosha) plot(Lot$pop, Lot$sales, xlab = &quot;population&quot;, ylab = &quot;sales&quot;) text(5000, 24000, &quot;Kenosha&quot;) abline(model_blr, col=&quot;blue&quot;) abline(model_Kenosha, col=&quot;red&quot;) par(mfrow = c(1, 2)) qqnorm(residuals(model_blr), main = &quot;&quot;) qqline(residuals(model_blr)) qqnorm(residuals(model_Kenosha), main = &quot;&quot;) qqline(residuals(model_Kenosha)) Submission Correctness Tests (SCT) test_error() test_object(“model_blr1”, incorrect_msg = “The basic linear regression model is incorrectly specified.”) test_object(“model_Kenosha”, incorrect_msg = “The linear regression model without Kenosha is incorrectly specified.”) success_msg(“Congratulations! Just because an observation is unusual does not make it bad or noninformative. Kenosha is close to the Illinois border; residents from Illinois probably participate in the Wisconsin lottery thus effectively increasing the potential pool of sales in Kenosha. Although unusual, there is interesting information to be learned from this observation.”) "],
["multiple-linear-regression-mlr.html", "Chapter 3 Multiple Linear Regression (MLR) 3.1 Method of least squares 3.2 Foundations of multiple linear regresson 3.3 Statistical inference and multiple linear regresson 3.4 Binary variables 3.5 Categorical variables 3.6 General linear hypothesis", " Chapter 3 Multiple Linear Regression (MLR) Chapter description This chapter introduces linear regression in the case of several explanatory variables, known as multiple linear regression (MLR). Many basic linear regression concepts extend directly, including goodness of fit measures such as the coefficient of determination and inference using t-statistics. Multiple linear regression models provide a framework for summarizing highly complex, multivariate data. Because this framework requires only linearity in the parameters, we are able to fit models that are nonlinear functions of the explanatory variables, thus providing a wide scope of potential applications. 3.1 Method of least squares 3.1.1 Video (Exercise). Method of least squares 3.1.1.1 Learning Objectives In this module, you learn how to: Interpret correlation coefficients by visualizing a scatterplot matrix Fit a plane to data using the method of least squares Predict an observation using a least squares fitted plane 3.1.1.2 Video Overheads Overhead A. Demand for term life insurance “Who buys insurance and how much do they buy?” Companies have data on current customers How do get info on potential (new) customers? To understand demand, consider the Survey of Consumer Finances (SCF) This is a nationally representative sample that contains extensive information on potential U.S. customers. We study a random sample of 500 of the 4,519 households with positive income that were interviewed in the 2004 survey. We now focus on n = 275 households that purchased term life insurance Overhead B. Term life insurance summary statistics We study y = face, the amount that the company will pay in the event of the death of the named insured. We focus on k = 3 explanatory variables - annual income, - the number of years of education of the survey respondent and - the number of household members, numhh. The data suggest that income and face are skewed so we also introduce logarithmic versions. Overhead C. Summary statistics Term &lt;- read.csv(&quot;CSVData\\\\term_life.csv&quot;, header = TRUE) # PICK THE SUBSET OF THE DATA CORRESPONDING TO TERM PURCHASE Term1 &lt;- subset(Term, subset = face &gt; 0) str(Term1) head(Term1) library(Rcmdr) Term2 &lt;- Term1[, c(&quot;education&quot;, &quot;face&quot;, &quot;income&quot;, &quot;logface&quot;, &quot;logincome&quot;, &quot;numhh&quot;)] options(scipen = 100, digits = 4) summvar &lt;- numSummary(Term2, statistic = c(&quot;mean&quot;, &quot;sd&quot;, &quot;quantiles&quot;), quantiles = c(0, .5, 1)) summvar Overhead C. Scatter plots of income versus face in original and logarithmic units par(mfrow = c(1, 2)) plot(Term2$income, Term2$face, xlab = &quot;income&quot;, ylab = &quot;face&quot;) plot(Term2$logincome, Term2$logface, xlab = &quot;log&quot;, ylab = &quot;log face&quot;) Overhead D. Correlation table round(cor(Term2), digits=3) Overhead E. Scatterplot matrix Term3 &lt;- Term1[,c(&quot;numhh&quot;, &quot;education&quot;, &quot;logincome&quot;, &quot;logface&quot;)] pairs(Term3, upper.panel = NULL, gap = 0, cex.labels = 1.25) Overhead F. Visualizing a regression plane education &lt;- seq(3, 16, length = 15) logincome &lt;- seq(5, 15, length = 15) f &lt;- function(education,logincome){ r &lt;- 5 + 0.221*education + 0.354*logincome } logface &lt;- outer(education, logincome, f) persp(education, logincome, logface, theta = 30, phi = 30, expand = 0.5, ticktype = &quot;detailed&quot;) rm(education,logincome,logface) education &lt;- seq(3, 16, length = 15) logincome &lt;- seq(5, 15, length = 15) f &lt;- function(education,logincome){ r &lt;- 5 + 0.221*education + 0.354*logincome } logface &lt;- outer(education, logincome, f) persp(education, logincome, logface, theta = 30, phi = 30, expand = 0.5, ticktype = &quot;simple&quot;, #ticktype = &quot;detailed&quot;, # xlab = &quot;x1&quot;, ylab=&quot;x2&quot;,zlab=&quot;y&quot;, nticks = 1) rm(education,logincome,logface) Overhead G. Method of least squares For observation \\(\\{(y, x_1, \\ldots, x_k)\\}\\), the height of the regression plane is \\[b_0 + b_1 x_1 + \\cdots + b_k x_k .\\] Thus, \\(y - (b_0 + b_1 x_1 + \\cdots + b_k x_k)\\) represents the deviation. The sum of squared deviations is \\[SS(b_0, \\ldots, b_k) = \\sum (y - (b_0 + b_1 x_1 + \\cdots + b_k x_k))^2 .\\] The method of least squares – determine values of \\(b_0, \\ldots, b_k\\) that minimize \\(SS\\). Overhead H. Fit a multiple linear regression model Term_mlr &lt;- lm(logface ~ education + numhh + logincome, data = Term2) round(coefficients(Term_mlr), digits=4) newdata &lt;- data.frame(logincome = log(60000), education = 12, numhh = 3) exp(predict(Term_mlr, newdata)) 3.1.2 Exercise. Least squares and term life data Assignment Text The prior video introduced the Survey of Consumer Finances (SCF) term life data. A subset consisting of only those who purchased term life insurance, has already been read into a dataframe Term2. Suppose that you wish to predict the amount of term life insurance that someone will purchase but are uneasy about the education variable. The SCF education variable is the number of completed years of schooling and so 12 corresponds to completing high school in the US. Your sense is that, for purposes of purchasing life insurance, high school graduates and those that attend college should be treated the same. So, in this exercise, your will create a new variable, education1, that is equal to years of education for those with education less than or equal to 12 and is equal to 12 otherwise. Instructions Use the pmin() function to create the education1 variable as part of the Term2 dataframe. Check your work by examining summary statistics for the revised Term2 dataframe. Examine correlations for the revised dataframe. Using the method of least squares and the function lm(), fit a MLR model using logface as the dependent variables and using education, numhh, and logincome as explanatory variables. With this fitted model and the function predict(), predict the face amount of insurance that someone with income of 40,000, 11 years of education, and 4 people in the household would purchase. Hint Remember that your prediction is in log dollars so you need to exponentiate it to get the results in the original dollar units Pre-exercise code # Pre-exercise code #library(Rcmdr) Term &lt;- read.csv(&quot;CSVData\\\\term_life.csv&quot;, header = TRUE) #Term &lt;- read.csv(&quot;https://assets.datacamp.com/production/repositories/2610/datasets/efc64bc2d78cf6b48ad2c3f5e31800cb773de261/term_life.csv&quot;, header = TRUE) Term1 &lt;- subset(Term, subset = face &gt; 0) Term2 &lt;- Term1[, c(&quot;education&quot;, &quot;face&quot;, &quot;income&quot;, &quot;logface&quot;, &quot;logincome&quot;, &quot;numhh&quot;)] Sample_code `@sample_code` # Create the `education1` variable as part of the `Term2` dataframe. Term2$education1 &lt;- pmin(12, Term2$education) # Check your work by examining summary statistics for the revised `Term2` dataframe. summary(___) # Examine correlations for the revised dataframe. round(cor(___), digits=3) # Fit a MLR model using `logface` as the dependent variables and using `education`, `numhh`, and `logincome` as explanatory variables. Term_mlr2 &lt;- lm(logface ~ ___ + numhh + logincome, data = Term2) # Predict the face amount of insurance that someone with income of 40,000, 11 years of education, and 4 people in the household would purchase. newdata &lt;- data.frame(logincome = log(40000), education1 = 11, numhh = 4) exp(predict(___, newdata)) Solution # Solution Term2$education1 &lt;- pmin(12, Term2$education) #Rcmdr::numSummary(Term2, statistic = c(&quot;mean&quot;, &quot;sd&quot;, &quot;quantiles&quot;), quantiles = c(0, .5, 1)) summary(Term2) round(cor(Term2), digits=3) Term_mlr2 &lt;- lm(logface ~ education1 + numhh + logincome, data = Term2) newdata &lt;- data.frame(logincome = log(40000), education1 = 11, numhh = 4) exp(predict(Term_mlr2, newdata)) Submission Correctness Tests (SCT) test_error() test_object(“Term2”, incorrect_msg = “Is the new education variable properly defined using the pmin() function?”) test_object(“Term_mlr2”, incorrect_msg = “The MLR model is incorrectly specified.”) test_object(“newdata”, incorrect_msg = “The new data object is incorrectly specified.”) success_msg(“Congratulations! You now have experience fitting a regression plane and using this plane for predictions. Prediction is one of the key tasks of ‘predictive modeling.’ Well done!”) 3.1.3 Exercise. Interpreting coefficients as proportional changes Assignment Text In a previous exercise, you fit a MLR model using logface as the outcome variable and using education, numhh, and logincome as explanatory variables; the resulting fit is in the object Term_mlr. For this fit, the coefficient associated with education is 0.2064. We now wish to interpret this regression coefficient. The typical interpretation of coefficients in a regression model is as a partial slope. That is, for the coefficient \\(b_1\\) associated with \\(x_1\\), we interpret \\(b_1\\) to be amount that the expected outcome changes per unit change in \\(x_1\\), holding the other explanatory variables fixed. For the term life example, the units of the outcome are in logarithmic dollars. So, for small values of \\(b_1\\), we can interpret this to be a proportional change in dollars. Instructions Determine least square fitted values for several selected values of education, holding other explantory variables fixed. For this part of the demonstration, we used their mean values. Determine the proportional changes. Note the relation between these values from a discrete change approximation to the regression coefficient for education equal to 0.2064. Hint Pre-exercise code # Pre-exercise code Term &lt;- read.csv(&quot;CSVData\\\\term_life.csv&quot;, header = TRUE) #Term &lt;- read.csv(&quot;https://assets.datacamp.com/production/repositories/2610/datasets/efc64bc2d78cf6b48ad2c3f5e31800cb773de261/term_life.csv&quot;, header = TRUE) Term1 &lt;- subset(Term, subset = face &gt; 0) Term2 &lt;- Term1[, c(&quot;education&quot;, &quot;face&quot;, &quot;income&quot;, &quot;logface&quot;, &quot;logincome&quot;, &quot;numhh&quot;)] Term_mlr &lt;- lm(logface ~ education + numhh + logincome, data = Term2) summary(Term_mlr)$coefficients[,1] Sample_code `@sample_code` Term_mlr &lt;- lm(logface ~ education + numhh + logincome, data = Term2) summary(Term_mlr)$coefficients[,1] # Determine least square fitted values for several selected values of `education`, holding other explantory variables fixed. educ_predict &lt;- c(14,14.1,14.2,14.3) newdata1 &lt;- data.frame(logincome = mean(Term2$logincome), education = educ_predict, numhh = mean(Term2$numhh)) lsfits1 &lt;- predict(Term_mlr, newdata1) lsfits1 # Determine the proportional changes. Note the relation between these values from a discrete change approximation to the regression coefficient for `education` equal to 0.2064. lsfits1[2:4] - lsfits1[1:3] pchange_fits1 &lt;- exp(lsfits1[2:4] - lsfits1[1:3]) pchange_fits1 Solution # Solution educ_predict &lt;- c(14,14.1,14.2,14.3) newdata1 &lt;- data.frame(logincome = mean(Term2$logincome), education = educ_predict, numhh = mean(Term2$numhh)) lsfits1 &lt;- predict(Term_mlr, newdata1) lsfits1 lsfits1[2:4] - lsfits1[1:3] pchange_fits1 &lt;- exp(lsfits1[2:4] - lsfits1[1:3]) pchange_fits1 Submission Correctness Tests (SCT) test_error() test_object(“educ_predict”, incorrect_msg = “Check to see that values of the education predictor variable are properly coded.”) test_object(“newdata1”, incorrect_msg = “The new data object is incorrectly specified.”) test_object(“lsfits1”, incorrect_msg = “The predicted fits at different values of education are incorrectly specified.”) test_object(“pchange_fits1”, incorrect_msg = “The proportional changes at different values of education are incorrectly specified.”) success_msg(“Congratulations! From calculus, small changes in logarithmic values can be interpreted as proportional changes. This is the reason for using natural logarithms.”) 3.1.4 Exercise. Interpreting coefficients as elasticities Assignment Text In a previous exercise, you fit a MLR model using logface as the outcome variable and using education, numhh, and logincome as explanatory variables; the resulting fit is in the object Term_mlr. From this fit, the coefficient associated with logincome is 0.4935. We now wish to interpret this regression coefficient. The typical interpretation of coefficients in a regression model is as a partial slope. When both \\(x_1\\) and \\(y\\) are in logarithmic units, then we can interpret \\(b_1\\) to be ratio of two percentage changes, known as an elasticity in economics. Mathematically, we summarize this as \\[ \\frac{\\partial \\ln y}{\\partial \\ln x} = \\left(\\frac{\\partial y}{y}\\right) ~/ ~\\left(\\frac{\\partial x}{x}\\right) . \\] Instructions For several selected values of logincome, determine the corresponding proportional changes. Determine least square fitted values for several selected values of logincome, holding other explantory variables fixed. Determine the corresponding proportional changes for the fitted values. Calculate the ratio of proportional changes of fitted values to those for income. Note the relation between these values (from a discrete change approximation) to the regression coefficient for logincome equal to 0.4935. Hint When you calculate the ratio of proportional changes of fitted values to those for income, note the relation between these values (from a discrete change approximation) to the regression coefficient for logincome equal to 0.4935. Sample_code `@sample_code` Term_mlr &lt;- lm(logface ~ education + numhh + logincome, data = Term2) summary(Term_mlr)$coefficients[,1] # For several selected values of `logincome`, determine the corresponding proportional changes. logincome_pred &lt;- c(11,11.1,11.2,11.3) pchange_income &lt;- 100*(exp(logincome_pred[2:4])/exp(logincome_pred[1:3])-1) pchange_income # Determine least square fitted values for several selected values of `logincome`, holding other explantory variables fixed. newdata2 &lt;- data.frame(logincome = logincome_pred, education = mean(Term2$education), numhh = mean(Term2$numhh)) lsfits2 &lt;- predict(Term_mlr, newdata2) # Determine the corresponding proportional changes for the fitted values. pchange_fits2 &lt;- 100*(exp(lsfits2[2:4])/exp(lsfits2[1:3])-1) pchange_fits2 # Calculate the ratio of proportional changes of fitted values to those for income. pchange_fits2/pchange_income Pre-exercise code # Pre-exercise code Term &lt;- read.csv(&quot;CSVData\\\\term_life.csv&quot;, header = TRUE) #Term &lt;- read.csv(&quot;https://assets.datacamp.com/production/repositories/2610/datasets/efc64bc2d78cf6b48ad2c3f5e31800cb773de261/term_life.csv&quot;, header = TRUE) Term1 &lt;- subset(Term, subset = face &gt; 0) Term2 &lt;- Term1[, c(&quot;education&quot;, &quot;face&quot;, &quot;income&quot;, &quot;logface&quot;, &quot;logincome&quot;, &quot;numhh&quot;)] Term_mlr &lt;- lm(logface ~ education + numhh + logincome, data = Term2) summary(Term_mlr)$coefficients[,1] Solution # Solution logincome_pred &lt;- c(11,11.1,11.2,11.3) pchange_income &lt;- 100*(exp(logincome_pred[2:4])/exp(logincome_pred[1:3])-1) pchange_income newdata2 &lt;- data.frame(logincome = logincome_pred, education = mean(Term2$education), numhh = mean(Term2$numhh)) lsfits2 &lt;- predict(Term_mlr, newdata2) pchange_fits2 &lt;- 100*(exp(lsfits2[2:4])/exp(lsfits2[1:3])-1) pchange_fits2 pchange_fits2/pchange_income Submission Correctness Tests (SCT) test_error() test_object(“logincome_pred”, incorrect_msg = “Check to see that values of the logarithmic income predictor variable are properly coded.”) test_object(“pchange_income”, incorrect_msg = “Check to see that the proportional changes of logarithmic income predictor variable are properly coded.”) test_object(“newdata2”, incorrect_msg = “The new data object is incorrectly specified.”) test_object(“lsfits2”, incorrect_msg = “The predicted fits at different values of logarithmic income are incorrectly specified.”) test_object(“pchange_fits2”, incorrect_msg = “The proportional changes at different values of logarithmic income are incorrectly specified.”) success_msg(“Congratulations! When both \\(x_1\\) and \\(y\\) are in logarithmic units, then we can interpret \\(b_1\\) to be ratio of two percentage changes, known as an elasticity in economics.”) 3.2 Foundations of multiple linear regresson 3.2.1 Video (Exercise). Foundations of multiple linear regression 3.2.1.1 Learning Objectives In this module, you learn how to: Contrast the observable to the error representation of the model Describe the unbiasedness and determine the variance of least squares regression coefficients Motivate the least squares method using the Gauss-Markov and normality of estimators 3.2.1.2 Video Overheads Overhead A. xxx 3.2.2 Exercise. Multiple choice exercise on the theory… Maybeee Assignment Text Instructions # Pre-exercise code # Solution 3.3 Statistical inference and multiple linear regresson 3.3.1 Video (Exercise). Statistical inference and multiple linear regression 3.3.1.1 Learning Objectives In this module, you learn how to: Explain mean square error and residual standard error in terms of degrees of freedom Develop an ANOVA table and use it to derive the coefficient of determination Calculate and interpret the coefficient of determination adjusted for degrees of freedom Conduct a test of a regression coefficient Summarize regression coefficients using point and interval estimators 3.3.1.2 Video Overheads Overhead A. Goodness of fit Summarize deviations \\(s^2\\) \\(R^2\\) \\(R_a^2\\) ANOVA table Overhead B. Goodness of fit and term life Term_mlr &lt;- lm(logface ~ education + numhh + logincome, data = Term2) summary(Term_mlr) anova(Term_mlr) Overhead C. Statistical inference hypothesis testing of a regression coefficient confidence intervals Overhead D. Statistical inference and term life Term_mlr &lt;- lm(logface ~ education + numhh + logincome, data = Term2) model_sum &lt;- summary(Term_mlr) model_sum$coefficients round(Rcmdr::Confint(Term_mlr, level = .95), digits = 3) round(confint(Term_mlr, level = .95), digits = 3) 3.3.2 Exercise. Statistical inference and term life Assignment Text In later chapters, we will learn how to specify a model using diagnostics techniques; these techniques were used to specify face in log dollars for the outcome and similarly income in log dollars as an explanatory variable. Just to see how things work, in this exercise we will create new variables face and income that are in the original units and run a regression with these. We have already seen that rescaling by constants do not affect relationships but can be helpful with interpretations, so we define both face and income to be in thousands of dollars. A prior video introduced the term life dataframe Term2. Instructions Create Term2$face by exponentiating logface and dividing by 1000. For convenience, we are storing this variable in the data set Term2. Use the same process to create Term2$income. Run a regression using face as the outcome variable and education, numhh, and income as explanatory variables. Summarize this model and identify the residual standard error (\\(s\\)) as well as the coefficient of determination (\\(R^2\\)) and the version adjusted for degrees of freedom (\\(R_a^2\\)). Hint Pre-exercise code # Pre-exercise code Term &lt;- read.csv(&quot;CSVData\\\\term_life.csv&quot;, header = TRUE) #Term &lt;- read.csv(&quot;https://assets.datacamp.com/production/repositories/2610/datasets/efc64bc2d78cf6b48ad2c3f5e31800cb773de261/term_life.csv&quot;, header = TRUE) Term1 &lt;- subset(Term, subset = face &gt; 0) Term2 &lt;- Term1[, c(&quot;education&quot;, &quot;face&quot;, &quot;income&quot;, &quot;logface&quot;, &quot;logincome&quot;, &quot;numhh&quot;)] Sample_code `@sample_code` # Create `Term2$face` and `Term2$income` Term2$face &lt;- exp(___)/___ Term2$income &lt;- exp(___)/___ # Run a regression using `face` as the outcome variable and `education`, `numhh`, and `income` as explanatory variables. Term_mlr1 &lt;- lm(face ~ ___, data = Term2) # Summarize this model summary(Term_mlr1) Solution # Solution Term2$face &lt;- exp(Term2$logface)/1000 Term2$income &lt;- exp(Term2$logincome)/1000 Term_mlr1 &lt;- lm(face ~ education + numhh + income, data = Term2) summary(Term_mlr1) Submission Correctness Tests (SCT) success_msg(“Congratulations! Compare these goodness of fit measures to those where income and face are in logarithmic units. Although not the only indicators, you will see that the proportion of variability explained (R square) and the statistical significance of coefficients are strikingly higher in the model with variables in logged units.”) 3.4 Binary variables 3.4.1 Video (Exercise). Binary variables 3.4.1.1 Learning Objectives In this module, you learn how to: Interpret regression coefficients associated with binary variables Use binary variables and interaction terms to create regression models that are nonlinear in the covariates 3.4.1.2 Video Overheads Overhead A. Binary variables We can define a new variable \\[ single= \\left\\{ \\begin{array}{ll} 0 &amp; \\text{for other respondents} \\\\ 1 &amp; \\text{for single respondents} \\end{array} \\right. \\] The variable single is said to be an indicator, or dummy, variable. To interpret coefficients, we now consider the regression function \\[\\text{E }logface = \\beta_0 + \\beta_1 logincome + \\beta_2 single \\] - This can be expressed as two lines \\[ \\text{E }logface = \\left\\{ \\begin{array}{ll} \\beta_0 + \\beta_1 logincome &amp; \\textrm{for other respondents} \\\\ \\beta_0 + \\beta_2 + \\beta_1 logincome &amp; \\textrm{for single respondents} \\end{array} \\right. . \\] - The least squares method of calculating the estimators, and the resulting theoretical properties, are the still valid when using binary variables. Overhead B. Visualize effect of binary variables Overhead C. R script for visualization Term4 &lt;- Term1[,c(&quot;numhh&quot;, &quot;education&quot;, &quot;logincome&quot;, &quot;logface&quot;, &quot;marstat&quot;)] Term4$marstat &lt;- as.factor(Term4$marstat) table(Term4$marstat) Term4$single &lt;- 1*(Term4$marstat == 0) model_single &lt;- lm(logface ~ logincome + single, data = Term4) summary(model_single) plot(Term4$logincome,Term4$logface,xlab=&quot;logarithmic income&quot;, ylab=&quot;log face&quot;, pch= 1+16*Term4$single, col = c(&quot;red&quot;, &quot;black&quot;, &quot;black&quot;)[Term4$marstat]) Ey1 &lt;- model_single$coefficients[1]+model_single$coefficients[2]*Term4$logincome Ey2 &lt;- Ey1 + model_single$coefficients[3] lines(Term4$logincome, Ey1) lines(Term4$logincome, Ey2, col=&quot;red&quot;) Overhead D. Interaction Terms Linear regression models are defined in terms of linear combinations of explanatory varibles but we can expand their scope through nonlinear transformations One type of nonlinear transform is the product of two varibles that is used to create what is known as an interaction variable To interpret coefficients, we now consider the regression function \\[\\text{E }logface = \\beta_0 + \\beta_1 logincome + \\beta_2 single + \\beta_3 single*logincome \\] - This can be expressed as two lines with different slopes \\[ \\text{E }logface = \\left\\{ \\begin{array}{ll} \\beta_0 + \\beta_1 logincome &amp; \\textrm{for other respondents} \\\\ \\beta_0 + \\beta_2 + (\\beta_1 + \\beta_3) logincome &amp; \\textrm{for single respondents} \\end{array} \\right. . \\] Overhead E. Visualizing binary variables with interactions terms Term4 &lt;- Term1[,c(&quot;numhh&quot;, &quot;education&quot;, &quot;logincome&quot;, &quot;logface&quot;, &quot;marstat&quot;)] Term4$marstat &lt;- as.factor(Term4$marstat) table(Term4$marstat) Term4$single &lt;- 1*(Term4$marstat == 0) model_single_inter &lt;- lm(logface ~ logincome + single + single*logincome, data = Term4) summary(model_single_inter) plot(Term4$logincome,Term4$logface,xlab=&quot;logarithmic income&quot;, ylab=&quot;log face&quot;, pch= 1+16*Term4$single, col = c(&quot;red&quot;, &quot;black&quot;, &quot;black&quot;)[Term4$marstat]) Ey1 &lt;- model_single_inter$coefficients[1]+model_single_inter$coefficients[2]*Term4$logincome Ey2 &lt;- Ey1 + model_single_inter$coefficients[3]+model_single_inter$coefficients[4]*Term4$logincome lines(Term4$logincome, Ey1) lines(Term4$logincome, Ey2, col=&quot;red&quot;) 3.4.2 Exercise. Binary variables and term life Assignment Text In the prior video, we saw how the variable single can be used with logarithmic income to explain logarithmic face amounts of term life insurance that people purchase. The coefficient associated with this variable turns out to be negative which is intuitively appealing; if an individual is single, then that person may not have the strong need to purchase financial security for others in the event of unexpected death. In this exercise, we will extend this by incorporating single into our larger regression model that contains other explanatory varibles, logincome, education and numhh. The data have been pre-loaded into the dataframe Term4. Instructions Calculate a table of correlation coefficients to examine pairwise linear relationships among the variables numhh, education, logincome, single, and logface. Fit a MLR model of logface using explanatory variables numhh, education, logincome, and single. Examine the residual standard deviation \\(s\\), the coefficient of determination \\(R^2\\), and the adjusted version \\(R_a^2\\). Also note the statistical significance of the coefficient associated with single. Repeat the MLR model fit while adding the interaction term single*logincome. Hint Pre-exercise code # Pre-exercise code Term &lt;- read.csv(&quot;CSVData\\\\term_life.csv&quot;, header = TRUE) #Term &lt;- read.csv(&quot;https://assets.datacamp.com/production/repositories/2610/datasets/efc64bc2d78cf6b48ad2c3f5e31800cb773de261/term_life.csv&quot;, header = TRUE) Term1 &lt;- subset(Term, subset = face &gt; 0) Term4 &lt;- Term1[,c(&quot;numhh&quot;, &quot;education&quot;, &quot;logincome&quot;, &quot;marstat&quot;, &quot;logface&quot;)] Term4$single &lt;- 1*(Term4$marstat == 0) Sample_code `@sample_code` # Calculate a table of correlation coefficients round(___(Term4[,c(&quot;numhh&quot;, &quot;education&quot;, &quot;logincome&quot;, &quot;single&quot;, &quot;logface&quot;)]), digits = 3) # Fit a MLR model of `logface` using explanatory variables `numhh`, `education`, `logincome`, and `single`. Term_mlr3 &lt;- lm(logface ~ education + numhh + logincome + single, data = Term4) summary(Term_mlr3) # Repeat the MLR model fit while adding the interaction term `single*logincome`. Term_mlr4 &lt;- lm(logface ~ education + numhh + logincome + single + single*logincome, data = Term4) summary(Term_mlr4) Solution # Solution round(cor(Term4[,c(&quot;numhh&quot;, &quot;education&quot;, &quot;logincome&quot;, &quot;single&quot;, &quot;logface&quot;)]), digits = 3) Term_mlr3 &lt;- lm(logface ~ education + numhh + logincome + single, data = Term4) summary(Term_mlr3) Term_mlr4 &lt;- lm(logface ~ education + numhh + logincome + single + single*logincome, data = Term4) summary(Term_mlr4) Submission Correctness Tests (SCT) success_msg(“Congratulations! From a correlation table, you saw that there are relationships with among explanatory variables and so it is not clear whether adding single to the model would be helpful. You explored this by first fitting a model by just adding the binary variable single, examined summary statistics, and checked the significance of the variable. Then, you explored the utility of the interaction of single with logarithmic income. Well done!”) 3.5 Categorical variables 3.5.1 Video (Exercise). Categorical variables 3.5.1.1 Learning Objectives In this module, you learn how to: Represent categorical variables using a set of binary variables Interpret the regression coefficients associated with categorical variables Describe the effect of the reference level choice on the model fit 3.5.1.2 Video Overheads Overhead A. Categorical variables Categorical variables provide labels for observations to denote membership in distinct groups, or categories. A binary variable is a special case of a categorical variable. To illustrate, a binary variable may tell us whether or not someone has health insurance. A categorical variable could tell us whether someone has (i) private individual health insurance, (ii) private group insurance, (iii) public insurance or (iv) no health insurance. For categorical variables, there may or may not be an ordering of the groups. For health insurance, it is difficult to say which is ‘larger’, private individual versus public health insurance (such as Medicare). However, for education, we may group individuals from a dataset into ‘low’, ‘intermediate’ and ‘high’ years of education. Factor is another term used for a (unordered) categorical explanatory variable. Overhead B. Term life example We studied y = logface, the amount that the company will pay in the event of the death of the named insured (in logarithmic dollars), focusing on the explanatory variables logincome, education, and numhh. We now supplement this by including the categorical variable, marstat, that is the marital status of the survey respondent. This may be: 1, for married 2, for living with partner 0, for other (SCF actually breaks this category into separated, divorced, widowed, never married and inapplicable, for persons age 17 or less or no further persons) Overhead C. Term life boxplots # Pre-exercise code Term &lt;- read.csv(&quot;CSVData\\\\term_life.csv&quot;, header = TRUE) #Term &lt;- read.csv(&quot;https://assets.datacamp.com/production/repositories/2610/datasets/efc64bc2d78cf6b48ad2c3f5e31800cb773de261/term_life.csv&quot;, header = TRUE) Term1 &lt;- subset(Term, subset = face &gt; 0) Term4 &lt;- Term1[,c(&quot;numhh&quot;, &quot;education&quot;, &quot;logincome&quot;, &quot;marstat&quot;, &quot;logface&quot;)] Term4$single &lt;- 1*(Term4$marstat == 0) Term4$marstat&lt;- as.factor(Term4$marstat) boxplot(logface ~ marstat, ylab = &quot;log face&quot;, xlab = &quot;Marital Status&quot;, data = Term4) table(Term4$marstat) # SUMMARY BY LEVEL OF MARSTAT library(Rcmdr) numSummary(Term4[, &quot;logface&quot;], groups = Term4$marstat, statistics = c(&quot;mean&quot;, &quot;sd&quot;)) numSummary(Term4[, &quot;logface&quot;], statistics = c(&quot;mean&quot;, &quot;sd&quot;)) Overhead D. Regression with a categorical variable Term4$marstat &lt;- as.factor(Term4$marstat) Term4$marstat &lt;- relevel(Term4$marstat, ref = &quot;2&quot;) summary(lm(logface ~ logincome+education+numhh+marstat, data = Term4)) Overhead E. t-ratios depend on the reference level \\[\\begin{array}{l|rr|rr|rr} \\hline &amp; \\text{Model 1}&amp;&amp; \\text{Model 2}&amp;&amp; \\text{Model 3}&amp;\\\\ \\hline \\text{Var}&amp; \\text{Coef} &amp; \\text{t-stat} &amp; \\text{Coef} &amp; \\text{t-stat} &amp;\\text{Coef} &amp; \\text{t-stat} \\\\\\hline logincome &amp; 0.452 &amp; 5.74 &amp; 0.452 &amp; 5.74 &amp; 0.452 &amp; 5.74 \\\\ education &amp;0.205 &amp; 5.30 &amp;0.205 &amp; 5.30&amp;0.205 &amp; 5.30 \\\\ numhh &amp; 0.248 &amp; 3.57 &amp; 0.248 &amp; 3.57 &amp; 0.248 &amp; 3.57 \\\\\\hline \\text{Intercept} &amp; 3.395 &amp; 3.77 &amp; 2.605&amp; 2.74 &amp; 2.838 &amp; 3.34\\\\ \\text{mar=0} &amp; -0.557 &amp; -2.15&amp; 0.232 &amp; 0.44\\\\ \\text{mar=1} &amp; &amp; &amp; 0.789 &amp; 1.59 &amp; 0.557 &amp; 2.15\\\\ \\text{mar=2}&amp; -0.789 &amp; -1.59 &amp; &amp; &amp; -0.232 &amp; -0.44\\\\ \\hline \\end{array}\\] 3.5.2 Exercise. Categorical variables and Wisconsin hospital costs Assignment Text This exercise examines the impact of various predictors on hospital charges. Identifying predictors of hospital charges can provide direction for hospitals, government, insurers and consumers in controlling these variables that in turn leads to better control of hospital costs. The data, from 1989, are aggregated by: drg, diagnostic related groups of costs, payer, type of health care provider (Fee for service, HMO, and other), and hsa, nine major geographic areas in Wisconsin. Some preliminary analysis of the data has already been done. In this exercise, we will analyze logcharge, the logarithm of total hospital charges per number of discharges, in terms of log_numdschg, the logarithm of the number of discharges. In the dataframe Hcost which has been loaded in advance, we restrict consideration to three types of drgs, numbers 209, 391, and 431. Instructions Fit a basic linear regression model using logarithmic number of discharges to predict logarithmic hospital costs and superimposed the fitted regression line on the scatter plot. Produce a scatter plot of logarithmic number of discharges to predict logarithmic hospital costs. Allow plotting symbols and colors to vary by diagnostic related group. Fit a MLR model using logarithmic number of discharges to predict logarithmic hospital costs, allowing intercepts and slopes to vary by diagnostic related groups. Superimpose the fits from the MLR model on the scatter plot of logarithmic number of discharges to predict logarithmic hospital costs. Hint Pre-exercise code # Pre-exercise code Hcost &lt;- read.csv(&quot;CSVData\\\\WiscHcosts.csv&quot;, header = TRUE) #Hcost &lt;- read.csv(&quot;https://assets.datacamp.com/production/repositories/2610/datasets/2cc1e2739bf827093db31d7c4e6dcdc348ac984e/WiscHcosts.csv&quot;, header = TRUE) Hcost1 &lt;- subset(Hcost, drg == 209|drg == 391|drg == 430) Sample_code `@sample_code` # Fit a basic linear regression model using logarithmic number of discharges to predict logarithmic hospital costs and superimposed the fitted regression line on the scatter plot. hosp_blr &lt;- lm(logcharge~log_numdschg, data=Hcost1) plot(logcharge~log_numdschg, data=Hcost1, xlab = &quot;log number discharges&quot;, ylab = &quot;log charge&quot;) abline(hosp_blr, col=&quot;red&quot;) # Produce a scatter plot of logarithmic number of discharges to predict logarithmic hospital costs. Allow plotting symbols and colors to vary by diagnostic related group. plot(logcharge~log_numdschg, data=Hcost1, xlab = &quot;log number discharges&quot;, ylab = &quot;log charge&quot;, pch= as.numeric(as.factor(Hcost1$drg)), col = c(&quot;red&quot;, &quot;black&quot;, &quot;blue&quot;)[as.factor(Hcost1$drg)]) legend(&quot;left&quot;, legend=c(&quot;drg 209&quot;,&quot;drg 391&quot;, &quot;drg 430&quot;), col=c(&quot;red&quot;, &quot;black&quot;, &quot;blue&quot;), pch = c(1,2,3)) # Fit a MLR model allowing intercepts and slopes to vary by drg. hosp_mlr &lt;- lm(logcharge~log_numdschg + as.factor(drg)*log_numdschg, data=Hcost1) # Superimpose the fits from the MLR model on the scatter plot of logarithmic number of discharges to predict logarithmic hospital costs. plot(logcharge~log_numdschg, data=Hcost1, xlab = &quot;log number discharges&quot;, ylab = &quot;log charge&quot;, pch= as.numeric(as.factor(Hcost1$drg)), col = c(&quot;red&quot;, &quot;black&quot;, &quot;blue&quot;)[as.factor(Hcost1$drg)]) xseq &lt;- seq(0,10,length.out=100) coef &lt;- summary(hosp_mlr)$coefficients[,1] fit209 &lt;- coef[1] + coef[2]*xseq lines(xseq,fit209, col=&quot;red&quot;) fit391 &lt;- coef[1] + coef[3] + (coef[2] + coef[5])*xseq lines(xseq,fit391, col=&quot;black&quot;) fit430 &lt;- coef[1] + coef[4] + (coef[2] + coef[6])*xseq lines(xseq,fit430, col=&quot;blue&quot;) Solution # Solution #par(mfrow = c(2, 1)) hosp_blr &lt;- lm(logcharge~log_numdschg, data=Hcost1) plot(logcharge~log_numdschg, data=Hcost1, xlab = &quot;log number discharges&quot;, ylab = &quot;log charge&quot;) abline(hosp_blr, col=&quot;red&quot;) plot(logcharge~log_numdschg, data=Hcost1, xlab = &quot;log number discharges&quot;, ylab = &quot;log charge&quot;, pch= as.numeric(as.factor(Hcost1$drg)), col = c(&quot;red&quot;, &quot;black&quot;, &quot;blue&quot;)[as.factor(Hcost1$drg)]) legend(&quot;left&quot;, legend=c(&quot;drg 209&quot;,&quot;drg 391&quot;, &quot;drg 430&quot;), col=c(&quot;red&quot;, &quot;black&quot;, &quot;blue&quot;), pch = c(1,2,3)) hosp_mlr &lt;- lm(logcharge~log_numdschg + as.factor(drg)*log_numdschg, data=Hcost1) #summary(hosp_mlr)$coefficients[,1] plot(logcharge~log_numdschg, data=Hcost1, xlab = &quot;log number discharges&quot;, ylab = &quot;log charge&quot;, pch= as.numeric(as.factor(Hcost1$drg)), col = c(&quot;red&quot;, &quot;black&quot;, &quot;blue&quot;)[as.factor(Hcost1$drg)]) xseq &lt;- seq(0,10,length.out=100) coef &lt;- summary(hosp_mlr)$coefficients[,1] fit209 &lt;- coef[1] + coef[2]*xseq lines(xseq,fit209, col=&quot;red&quot;) fit391 &lt;- coef[1] + coef[3] + (coef[2] + coef[5])*xseq lines(xseq,fit391, col=&quot;black&quot;) fit430 &lt;- coef[1] + coef[4] + (coef[2] + coef[6])*xseq lines(xseq,fit430, col=&quot;blue&quot;) Submission Correctness Tests (SCT) success_msg(“Congratulations! When you superimposed the fits from the MLR model on the scatter plot of logarithmic number of discharges to predict logarithmic hospital costs, note how slopes differ dramatically from the slope from the basic linear regression model.”) 3.6 General linear hypothesis 3.6.1 Video (Exercise). Hypothesis testing 3.6.1.1 Learning Objectives In this module, you learn how to: Jointly test the significance of a set of regression coefficients using the general linear hypothesis Conduct a test of a regression coefficient versus one- or two-side alternatives 3.6.1.2 Video Overheads Overhead A. Testing the significance of a categorical variable Term &lt;- read.csv(&quot;CSVData\\\\term_life.csv&quot;, header = TRUE) #Term &lt;- read.csv(&quot;https://assets.datacamp.com/production/repositories/2610/datasets/efc64bc2d78cf6b48ad2c3f5e31800cb773de261/term_life.csv&quot;, header = TRUE) Term1 &lt;- subset(Term, subset = face &gt; 0) Term4 &lt;- Term1[,c(&quot;numhh&quot;, &quot;education&quot;, &quot;logincome&quot;, &quot;marstat&quot;, &quot;logface&quot;)] Term_mlr1 &lt;- lm(logface ~ logincome + education + numhh + as.factor(Term4$marstat), data = Term4) anova(Term_mlr1) Term_mlr2 &lt;- lm(logface ~ logincome + education + numhh, data = Term4) Fstat &lt;- (anova(Term_mlr2)$`Sum Sq`[4] - anova(Term_mlr1)$`Sum Sq`[5])/(2*anova(Term_mlr1)$`Mean Sq`[5]) Fstat cat(&quot;p-value is&quot;, 1 - pf(Fstat, df1 = 2 , df2 = anova(Term_mlr1)$Df[5])) Overhead B. Overview of the general linear hypothesis The likelihood ratio is a general statistical test procedure that compares a model to a subset The general linear hypothesis test procedure is similar. Start with a (large) linear regression model, examine the fit to a set of data Compare this to smaller model that is a subset of the large model. “Subset” is the sense that regression coefficients from the small model are linear combinations of regression coefficients of the large model (e.g., set them to zero) Although the likelihood ratio test is more generally available, the general linear hypothesis test is more accurate for smaller data sets (for normally distributed data) Overhead C. Procedure for conducting the general linear hypothesis Run the full regression and get the error sum of squares and mean square error, which we label as \\((Error SS)_{full}\\) and \\(s^2_{full}\\), respectively. Run a reduced regression and get the error sum of squares, labelled \\((Error SS)_{reduced}\\). Using \\(p\\) for the number of linear restrictions, calculate \\[ F-ratio = \\frac{(Error SS)_{reduced}-(Error SS)_{full}}{p s^2_{full}} . \\] The probability value is \\(p-value = \\Pr(F_{p,df} &gt; F-ratio)\\) where \\(F_{p,df}\\) has an F distribution with degrees of freedom p and df, respectively. (Here, df is the degrees of freedom for the full model.) Overhead D. The general linear hypothesis for a single variable Suppose that you wish to test the hypothesisthat a regression coefficient equals 0. One could use the general linear hypothsis procedure with \\(p=1\\). One could also examine the corresponding \\(t-ratio\\). Which is correct? Both. One can show that \\((t-ratio)^2 = F-ratio\\), so they are equivalent statistics. The general linear hypothesis is useful because it can be extended to multiple coefficients. The t-ratio is useful because it can be used to examine one-sided alternative hypotheses. 3.6.2 Exercise. Hypothesis testing and term life Assignment Text With our Term life data, let us compare a model based on the binary variable that indicates whether a survey respondent is single versus the more complex marital status, marstat. In principle, more detailed information is better. But, it may be that the additional information in marstat, compared to single, does not help fit the data in a significantly better way. As part of the preparatory work, the dataframe Term4 is available that includes the binary variable single and the factor marstat. Moreover, the regression object Term_mlr contains information in a multiple linear regression fit of logface on the base explanatory variables ’logincome,education, andnumhh`. Instructions Fit a MLR model using the base explanatory variables plus single and another model using the base variables plus marstat. Use the F test to decide whether the additional complexity marstat is warranted by calculating the p-value associated with this test. Fit a MLR model using the base explanatory variables plus single interacted with logincome and another model using the base variables plus marstat interacted with logincome. Use the F test to decide whether the additional complexity marstat is warranted by calculating the p-value associated with this test. Hint Here is the code to calculate it by hand Fstat12 &lt;- (anova(Term_mlr1)$`Sum Sq`[5] - anova(Term_mlr2)$`Sum Sq`[5])/(1*anova(Term_mlr2)$`Mean Sq`[5]) Fstat12 cat(&quot;p-value is&quot;, 1 - pf(Fstat12, df1 = 1 , df2 = anova(Term_mlr2)$Df[5])) Pre-exercise code # Pre-exercise code Term &lt;- read.csv(&quot;CSVData\\\\term_life.csv&quot;, header = TRUE) #Term &lt;- read.csv(&quot;https://assets.datacamp.com/production/repositories/2610/datasets/efc64bc2d78cf6b48ad2c3f5e31800cb773de261/term_life.csv&quot;, header = TRUE) Term1 &lt;- subset(Term, subset = face &gt; 0) Term4 &lt;- Term1[,c(&quot;numhh&quot;, &quot;education&quot;, &quot;logincome&quot;, &quot;marstat&quot;, &quot;logface&quot;)] Term4$single &lt;- 1*(Term4$marstat == 0) Term4$marstat &lt;- as.factor(Term4$marstat) Term_mlr &lt;- lm(logface ~ logincome + education + numhh , data = Term4) anova(Term_mlr) Sample_code `@sample_code` # Fit a MLR model using the base explanatory variables plus `single` and another model using the base variables plus `marstat`. Term_mlr1 &lt;- lm(logface ~ logincome + education + numhh +single, data = Term4) Term_mlr2 &lt;- lm(logface ~ logincome + education + numhh +marstat, data = Term4) # Use the F test to decide whether the additional complexity `marstat` is warranted by calculating the p-value associated with this test. anova(Term_mlr1,Term_mlr2) # Fit a MLR model using the base explanatory variables plus `single` interacted with `logincome` and another model using the base variables plus `marstat` interacted with `logincome`. Term_mlr3 &lt;- lm(logface ~ logincome + education + numhh + single*logincome, data = Term4) Term_mlr4 &lt;- lm(logface ~ logincome + education + numhh +marstat*logincome, data = Term4) # Use the F test to decide whether the additional complexity `marstat` is warranted by calculating the p-value associated with this test. anova(Term_mlr3,Term_mlr4) Solution # Solution Term_mlr1 &lt;- lm(logface ~ logincome + education + numhh +single, data = Term4) Term_mlr2 &lt;- lm(logface ~ logincome + education + numhh +marstat, data = Term4) anova(Term_mlr1,Term_mlr2) #Fstat12 &lt;- (anova(Term_mlr1)$`Sum Sq`[5] - # anova(Term_mlr2)$`Sum Sq`[5])/(1*anova(Term_mlr2)$`Mean Sq`[5]) #Fstat12 #cat(&quot;p-value is&quot;, 1 - pf(Fstat12, df1 = 1 , df2 = anova(Term_mlr2)$Df[5])) Term_mlr3 &lt;- lm(logface ~ logincome + education + numhh + single*logincome, data = Term4) Term_mlr4 &lt;- lm(logface ~ logincome + education + numhh +marstat*logincome, data = Term4) anova(Term_mlr3,Term_mlr4) Submission Correctness Tests (SCT) success_msg(“Congratulations! Hypothesis testing is a primary tool for ‘inferring’ about the real world {in contrast to mathematical ‘deduction’.} Moreover, as we will see in the next chapter, it can also be used to develop a model.”) 3.6.3 Exercise. Hypothesis testing and Wisconsin hospital costs Assignment Text In a previous exercise, you were introduced to a dataset with hospital charges aggregated by: drg, diagnostic related groups of costs, payer, type of health care provider (Fee for service, HMO, and other), and hsa, nine major geographic areas. We continue our analysis of the outcome variable logcharge, the logarithm of total hospital charges per number of discharges, in terms of log_numdschg, the logarithm of the number of discharges, as well as the three categorical variables used in the aggregation. As before, we restrict consideration to three types of drgs, numbers 209, 391, and 431 that has been preloaded in the dataframe Hcost1. Instructions Fit a basic linear regression model using logarithmic hospital costs as the outcome variable and explanatory variable logarithmic number of discharges. Fit a MLR model using logarithmic hospital costs as the outcome variable and explanatory variables logarithmic number of discharges and the categorical variable diagnostic related group. Identify the F statistic and p value that test the importance of diagnostic related group. Fit a MLR model using logarithmic hospital costs as the outcome variable and explanatory variable logarithmic number of discharges interacted with diagnostic related group. Identify the F statistic and p value that test the importance of diagnostic related group interaction with logarithmic number of discharges. Calculate a coefficient of determination, \\(R^2\\), for each of these models as well as for a model using logarithmic number of discharges and categorical variable hsa as predictors. Hint Pre-exercise code # Pre-exercise code Hcost &lt;- read.csv(&quot;CSVData\\\\WiscHcosts.csv&quot;, header = TRUE) #Hcost &lt;- read.csv(&quot;https://assets.datacamp.com/production/repositories/2610/datasets/2cc1e2739bf827093db31d7c4e6dcdc348ac984e/WiscHcosts.csv&quot;, header = TRUE) Hcost1 &lt;- subset(Hcost, drg == 209|drg == 391|drg == 430) Sample_code `@sample_code` # Regress log charges on log number of discharges hosp_blr &lt;- lm(logcharge ~ log_numdschg , data=Hcost1) anova(hosp_blr) # Regress log charges on log number of discharges and drg. Identify the *F* statistic and *p* value that test the importance of diagnostic related group. hosp_mlr1 &lt;- lm(logcharge ~ log_numdschg + as.factor(drg), data=Hcost1) anova(hosp_mlr1) # Regress log charges on the interaction of log number of discharges and drg. hosp_mlr2 &lt;- lm(logcharge ~ log_numdschg + as.factor(drg)*log_numdschg, data=Hcost1) anova(hosp_mlr2) # Calculate a coefficient of determination, $R^2$, for each of these models as well as for a model using logarithmic number of discharges and categorical variable `hsa` as predictors. summary(hosp_blr)$r.squared summary(hosp_mlr1)$r.squared summary(hosp_mlr2)$r.squared hosp_mlr3 &lt;- lm(logcharge ~ log_numdschg + as.factor(hsa)*log_numdschg, data=Hcost1) summary(hosp_mlr3)$r.squared Solution # Solution hosp_blr &lt;- lm(logcharge ~ log_numdschg , data=Hcost1) anova(hosp_blr) hosp_mlr1 &lt;- lm(logcharge ~ log_numdschg + as.factor(drg), data=Hcost1) anova(hosp_mlr1) hosp_mlr2 &lt;- lm(logcharge ~ log_numdschg + as.factor(drg)*log_numdschg, data=Hcost1) anova(hosp_mlr2) summary(hosp_blr)$r.squared summary(hosp_mlr1)$r.squared summary(hosp_mlr2)$r.squared hosp_mlr3 &lt;- lm(logcharge ~ log_numdschg + as.factor(hsa)*log_numdschg, data=Hcost1) summary(hosp_mlr3)$r.squared Submission Correctness Tests (SCT) success_msg(“Congratulations! By examining the coefficients of determination, \\(R^2\\), for each of these models, you see that this provides one piece of evidence that the hsa is a far poorer predictor of costs than drg.”) 3.6.4 Exercise. Hypothesis testing and auto claims Assignment Text As an actuarial analyst, you are working with a large insurance company to help them understand their claims distribution for their private passenger automobile policies. You have available claims data for a recent year, consisting of: state: codes 01 through 17 used, with each code randomly assigned to an actual individual state class: rating class of operator, based on age, gender, marital status, and use of vehicle gender: operator gender age: operator age paid: amount paid to settle and close a claim. You are focusing on older drivers, 50 and higher, for which there are n = 6,773 claims available. Instructions Run a regression of logpaid on age. Is age a statistically significant variable? To respond to this question, use a formal test of hypothesis. State your null and alternative hypotheses, decision-making criterion, and your decision-making rule. Also comment on the goodness of fit of this variable. Consider using class as a single explanatory variable. Use the one factor to estimate the model and respond to the following questions. b (i). What is the point estimate of claims in class C7, drivers 50-69, driving to work or school, less than 30 miles per week with annual mileage under 7500, in natural logarithmic units? b (ii). Determine the corresponding 95% confidence interval of expected claims, in natural logarithmic units. b (iii). Convert the 95% confidence interval of expected claims that you determined in part b(ii) to dollars. Run a regression of logpaid on age, gender and the categorical variables state and class. c (i). Is gender a statistically significant variable? To respond to this question, use a formal test of hypothesis. State your null and alternative hypotheses, decision-making criterion, and your decision-making rule. c (ii). Is class a statistically significant variable? To respond to this question, use a formal test of hypothesis. State your null and alternative hypotheses, decision-making criterion, and your decision-making rule. c (iii). Use the model to provide a point estimate of claims in dollars (not log dollars) for a male age 60 in STATE 2 in class C7. c (iv). Write down the coefficient associated with class C7 and interpret this coefficient. Submission Correctness Tests (SCT) success_msg(“Congratulations!”) "],
["variable-selection.html", "Chapter 4 Variable Selection 4.1 An iterative approach to data analysis and modeling 4.2 Automatic variable selection procedures 4.3 Residual analysis 4.4 Unusual observations 4.5 Collinearity 4.6 Selection criteria", " Chapter 4 Variable Selection Chapter description This chapter describes tools and techniques to help you select variables to enter into a linear regression model, beginning with an iterative model selection process. In applications with many potential explanatory variables, automatic variable selection procedures are available that will help you quickly evaluate many models. Nonetheless, automatic procedures have serious limitations including the inability to account properly for nonlinearities such as the impact of unusual points; this chapter expands upon the Chapter 2 discussion of unusual points. It also describes collinearity, a common feature of regression data where explanatory variables are linearly related to one another. Other topics that impact variable selection, including out-of-sample validation, are also introduced. # Reformat Data RSurvey &lt;- read.csv(&quot;CSVData\\\\RiskSurvey.csv&quot;, header = TRUE) str(RSurvey) RSurvey$firmcost &lt;- RSurvey$FIRMCOST RSurvey$assume &lt;- RSurvey$ASSUME RSurvey$cap &lt;- RSurvey$CAP RSurvey$logsize &lt;- RSurvey$SIZELOG RSurvey$indcost &lt;- RSurvey$INDCOST RSurvey$central &lt;- RSurvey$CENTRAL RSurvey$soph &lt;- RSurvey$SOPH RSurvey2 &lt;- RSurvey[,c(&quot;firmcost&quot;, &quot;assume&quot;, &quot;cap&quot;, &quot;logsize&quot;, &quot;indcost&quot;, &quot;central&quot;, &quot;soph&quot;)] str(RSurvey2) #write.csv(RSurvey2,&quot;CSVData\\\\Risk_survey.csv&quot;,row.names = FALSE) 4.1 An iterative approach to data analysis and modeling 4.1.1 Video (Exercise). An iterative approach to data analysis and modeling 4.1.1.1 Learning Objectives In this module, you learn how to: Describe the iterative approach to data analysis and modeling. 4.1.1.2 Video Overheads Overhead A. Iterative approach Model formulation stage Fitting Diagnostic checking - the data and model must be consistent with one another before additional inferences can be made. plot.new() par(mar=c(0,0,0,0), cex=0.9) plot.window(xlim=c(0,18),ylim=c(-5,5)) text(1,3,labels=&quot;DATA&quot;,adj=0, cex=0.8) text(1,0,labels=&quot;PLOTS&quot;,adj=0, cex=0.8) text(1,-3,labels=&quot;THEORY&quot;,adj=0, cex=0.8) text(3.9,0,labels=&quot;MODEL\\nFORMULATION&quot;,adj=0, cex=0.8) text(8.1,0,labels=&quot;FITTING&quot;,adj=0, cex=0.8) text(11,0,labels=&quot;DIAGNOSTIC\\nCHECKING&quot;,adj=0, cex=0.8) text(15,0,labels=&quot;INFERENCE&quot;,adj=0, cex=0.8) text(14.1,0.5,labels=&quot;OK&quot;,adj=0, cex=0.6) rect(0.8,2.0,2.6,4.0) arrows(1.7,2.0,1.7,1.0,code=2,lwd=2,angle=25,length=0.10) rect(0.8,-1.0,2.6,1.0) arrows(1.7,-2.0,1.7,-1.0,code=2,lwd=2,angle=25,length=0.10) rect(0.8,-4.0,2.6,-2.0) arrows(2.6,0,3.2,0,code=2,lwd=2,angle=25,length=0.10) x&lt;-c(5,7.0,5,3.2) y&lt;-c(2,0,-2,0) polygon(x,y) arrows(7.0,0,8.0,0,code=2,lwd=2,angle=25,length=0.10) rect(8.0,-1.0,9.7,1.0) arrows(9.7,0,10.2,0,code=2,lwd=2,angle=25,length=0.10) x1&lt;-c(12,14.0,12,10.2) y1&lt;-c(2,0,-2,0) polygon(x1,y1) arrows(14.0,0,14.8,0,code=2,lwd=2,angle=25,length=0.10) rect(14.8,-1.0,17.5,1.0) arrows(12,-2.0,12,-3,code=2,lwd=2,angle=25,length=0.10) arrows(12,-3.0,5,-3,code=2,lwd=2,angle=25,length=0.10) arrows(5,-3.0,5,-2,code=2,lwd=2,angle=25,length=0.10) Overhead B. Many possible models \\[\\begin{array}{l|r} \\hline \\text{E }y = \\beta _{0} &amp; \\text{1 model with no variables } \\\\ \\text{E }y = \\beta _{0}+\\beta_1 x_{i}, &amp; \\text{4 models with one variable} \\\\ \\text{E }y = \\beta _{0}+\\beta_1 x_{i}+\\beta_{2} x_{j}, &amp; \\text{6 models with two variables} \\\\ \\text{E }y = \\beta _{0}+\\beta_{1} x_{1}+\\beta_{2} x_{j} +\\beta_{3} x_{k},&amp; \\text{4 models with three variables} \\\\ \\text{E }y = \\beta_{0} + \\beta_{1} x_{1} + \\beta_{2} x_{2} +\\beta_{3} x_{3}+\\beta_{4} x_{4} &amp; \\text{1 model with all variables} \\\\ \\hline \\end{array}\\] With k explanatory variables, there are \\(2^k\\) possible linear models There are infinitely many nonlinear ones!! Overhead C. Model validation Model validation is the process of confirming our proposed model. Concern: data-snooping - fitting many models to a single set of data. Response to concern: out-of-sample validation. Divide the data into model development, or training and validation, or test, subsamples. par(mai=c(0,0.1,0,0)) plot.new() plot.window(xlim=c(0,18),ylim=c(-10,10)) rect(1,-1.2,14,1.2) rect(7,4,15,8) rect(1,-8,6,-4) x&lt;-seq(1.5,9,length=6) y&lt;-rep(0,6) text(x,y,labels=c(1:6),cex=1.5) x1&lt;-seq(10.5,11.5,length=3) y1&lt;-rep(0,3) text(x1,y1,labels=rep(&quot;.&quot;,3),cex=3) text(13,0,labels=&quot;n&quot;,cex=1.5) text(15,0,labels=&quot;ORIGINAL\\nSAMPLE\\nSIZE n&quot;,adj=0) text(7.5,6,labels=&quot;MODEL DEVELOPMENT\\nSUBSAMPLE SIZE&quot;,adj=0) text(12.5,5.3, expression(n[1]), adj=0, cex=1.1) text(1.4,-6,labels=&quot;VALIDATION\\nSUBSAMPLE\\nSIZE&quot;,adj=0) text(2.8,-7.2,expression(n[2]),adj=0, cex=1.1) arrows(1.8,0.8,8.3,3.9,code=2,lwd=2,angle=15,length=0.2) arrows(4.8,0.8,9,3.8,code=2,lwd=2,angle=15,length=0.2) arrows(9.1,0.9,9.5,3.8,code=2,lwd=2,angle=15,length=0.2) arrows(12.8,0.8,10,3.8,code=2,lwd=2,angle=15,length=0.2) arrows(2.9,-0.9,2.5,-3.8,code=2,lwd=2,angle=15,length=0.2) arrows(5.9,-0.9,3.1,-3.8,code=2,lwd=2,angle=15,length=0.2) arrows(7.4,-0.9,3.5,-3.8,code=2,lwd=2,angle=15,length=0.2) 4.1.2 MC Exercise. An iterative approach to data modeling Which of the following is not true? A. Diagnostic checking reveals symptoms of mistakes made in previous specifications. B. Diagnostic checking provides ways to correct mistakes made in previous specifications. C. Model formulation is accomplished by using prior knowledge of relationships. D. Understanding theoretical model properties is not really helpful when matching a model to data or inferring general relationships based on the data. 4.2 Automatic variable selection procedures 4.2.1 Video (Exercise). Automatic variable selection procedures 4.2.1.1 Learning Objectives In this module, you learn how to: Identify some examples of automatic variable selection procedures Describe the purpose of automatic variable selection procedures and their limitations Describe “data-snooping” 4.2.1.2 Video Overheads Overhead A. Classic stepwise regression algorithm Suppose that the analyst has identified one variable as the outcome, \\(y\\), and \\(k\\) potential explanatory variables, \\(x_1, x_2, \\ldots, x_k\\). (i). Consider all possible regressions using one explanatory variable. Choose the one with the highest t-statistic. (ii). Add a variable to the model from the previous step. The variable to enter is with the highest t-statistic. (iii). Delete a variable to the model from the previous step. Delete the variable with the small t-statistic if the statistic is less than, e.g., 2 in absolute value. (iv). Repeat steps (ii) and (iii) until all possible additions and deletions are performed. Overhead B. Drawbacks of stepwise regression The procedure “snoops” through a large number of models and may fit the data “too well.” There is no guarantee that the selected model is the best. The algorithm does not consider models that are based on nonlinear combinations of explanatory variables. It ignores the presence of outliers and high leverage points. Overhead C. Data-snooping in stepwise regression Generate \\(y\\) and \\(x_1 - x_{50}\\) using a random number generator By design, there is no relation between \\(y\\) and \\(x_1 - x_{50}\\). But, through stepwise regression, we “discover” a relationship that explains 14% of the variation!!! Call: lm(formula = y ~ xvar27 + xvar29 + xvar32, data = X) Coefficients: Estimate Std. Error t value Pr(&gt;|t|) (Intercept) -0.04885 0.09531 -0.513 0.6094 xvar27 0.21063 0.09724 2.166 0.0328 * xvar29 0.24887 0.10185 2.443 0.0164 * xvar32 0.25390 0.09823 2.585 0.0112 * Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Residual standard error: 0.9171 on 96 degrees of freedom Multiple R-squared: 0.1401, Adjusted R-squared: 0.1132 F-statistic: 5.212 on 3 and 96 DF, p-value: 0.002233 Overhead D. Variants of stepwise regression This uses the R function step() The option direction can be used to change how variables enter Forward selection. Add one variable at a time without trying to delete variables. Backwards selection. Start with the full model and delete one variable at a time without trying to add variables. The option scope can be used to specify which variables must be included Overhead E. Automatic variable selection procedures Stepwise regression is a type of automatic variable selection procedure. These procedures are useful because they can quickly search through several candidate models. They mechanize certain routine tasks and are excellent at discovering patterns in data. They are so good at detecting patterns that they analyst must be wary of overfitting (data-snooping) They can miss certain patterns (nonlinearities, unusual points) A model suggested by automatic variable selection procedures should be subject to the same careful diagnostic checking procedures as a model arrived at by any other means 4.2.2 Exercise. Data-snooping in stepwise regression Assignment Text Automatic variable selection procedures, such as the classic stepwise regression algorithm, are very good at detecting patterns. Sometimes they are too good in the sense that they detect patterns in the sample that are not evident in the population from which the data are drawn. The detect “spurious” patterns. This exercise illustrates this phenomenom by using a simulation, designed so that the outcome variable (y) and the explanatory variables are mutually independent. So, by design, there is no relationship between the outcome and the explanatory variables. As part of the code set-up, we have n = 100 observations generated of the outcome y and 50 explanatory variables, xvar1 through xvar50. As anticipated, collections of explanatory variables are not statistically significant. However, with the step() function, you will find some statistically significant relationships! Instructions Fit a basic linear regression model and MLR model with the first ten explanatory variables. Compare the models via an F test. Fit a multiple linear regression model with all fifty explanatory variables. Compare this model to the one with ten variables via an F test. Use the step function to find the best model starting with the fitted model containing all fifty explanatory variables and summarize the fit. Hint The code shows stepwise regression using BIC, a criterion that results in simpler models than AIC. For AIC, use the option k=2 in the [step()] function (the default) Pre-exercise code # Pre-exercise code set.seed(1237) X &lt;- as.data.frame(matrix(rnorm(100*50, mean = 0, sd = 1), ncol = 50)) colnames(X) &lt;- paste(&quot;xvar&quot;, 1:50, sep = &quot;&quot;) X$y &lt;- with(X, matrix(rnorm(100*1, mean = 0, sd = 1), ncol = 1)) #cor(X[,c(&quot;xvar1&quot;,&quot;xvar2&quot;,&quot;xvar3&quot;,&quot;xvar4&quot;,&quot;xvar5&quot;,&quot;xvar6&quot;,&quot;xvar7&quot;,&quot;xvar8&quot;,&quot;xvar9&quot;,&quot;xvar10&quot;,&quot;y&quot;)], use = &quot;complete.obs&quot;) Sample_code `@sample_code` # Fit a basic linear regression model and MLR model with the first ten explanatory variables. Compare the models via an *F* test. model_step1 &lt;- lm(y ~ xvar1, data = X) model_step10 &lt;- lm(y ~ xvar1 + xvar2 + xvar3 + xvar4 + xvar5 + xvar6 + xvar7 + xvar8 + xvar9 + xvar10, data = X) anova(___, ___) # Fit a multiple linear regression model with all fifty explanatory variables. Compare this model to the one with ten variables via an *F* test. model_step50 &lt;- lm(y ~ xvar1 + xvar2 + xvar3 + xvar4 + xvar5 + xvar6 + xvar7 + xvar8 + xvar9 + xvar10 + xvar11 + xvar12 + xvar13 + xvar14 + xvar15 + xvar16 + xvar17 + xvar18 + xvar19 + xvar20 + xvar21 + xvar22 + xvar23 + xvar24 + xvar25 + xvar26 + xvar27 + xvar28 + xvar29 + xvar30 + xvar31 + xvar32 + xvar33 + xvar34 + xvar35 + xvar36 + xvar37 + xvar38 + xvar39 + xvar40 + xvar41 + xvar42 + xvar43 + xvar44 + xvar45 + xvar46 + xvar47 + xvar48 + xvar49 + xvar50, data = X) anova(___, ___) # Use the `step` function, starting with the fitted model containing all fifty explanatory variables and summarize the fit. #For BIC: model_stepwise &lt;- step(___, data = X, direction= &quot;both&quot;, k = log(nrow(X)), trace = 0) summary(model_stepwise) Solution # Solution model_step1 &lt;- lm(y ~ xvar1, data = X) model_step10 &lt;- lm(y ~ xvar1 + xvar2 + xvar3 + xvar4 + xvar5 + xvar6 + xvar7 + xvar8 + xvar9 + xvar10, data = X) anova(model_step1,model_step10) model_step50 &lt;- lm(y ~ xvar1 + xvar2 + xvar3 + xvar4 + xvar5 + xvar6 + xvar7 + xvar8 + xvar9 + xvar10 + xvar11 + xvar12 + xvar13 + xvar14 + xvar15 + xvar16 + xvar17 + xvar18 + xvar19 + xvar20 + xvar21 + xvar22 + xvar23 + xvar24 + xvar25 + xvar26 + xvar27 + xvar28 + xvar29 + xvar30 + xvar31 + xvar32 + xvar33 + xvar34 + xvar35 + xvar36 + xvar37 + xvar38 + xvar39 + xvar40 + xvar41 + xvar42 + xvar43 + xvar44 + xvar45 + xvar46 + xvar47 + xvar48 + xvar49 + xvar50, data = X) anova(model_step10,model_step50) #For BIC: model_stepwise &lt;- step(model_step50, data = X, direction= &quot;both&quot;, k = log(nrow(X)), trace = 0) summary(model_stepwise) # An example with scope #model_step5a &lt;- step(model_step4, data = X, direction= &quot;both&quot;, k=log(nrow(X)), trace = 0, # scope = list(lower = ~xvar1+xvar2, upper = model_step4)) #summary(model_step5a) #For AIC: #step(model_step4, data = X, direction= &quot;both&quot;, k=2, trace = 0) # k=2 is by default Submission correctness test (SCT) success_msg(“Excellent! The step procedure repeatedly fits many models to a data set. We summarize each fit with hypothesis testing statistics like t-statistics and p-values. But, remember that hypothesis tests are designed to falsely detect a relationship a fraction of the time (typically 5%). For example, if you run a t-test 50 times (for each explanatory variable), you can expect to get two or three statistically significant explanatory variables even for unrelated variables (because 50 times 0.05 = 2.5).”) 4.3 Residual analysis 4.3.1 Video (Exercise). Residual analysis 4.3.1.1 Learning Objectives In this module, you learn how to: Explain how residual analysis can be used to improve a model specification Use relationships between residuals and potential explanatory variables to improve model specification 4.3.1.2 Video Overheads Overhead A. Residual analysis Use \\(e_i = y_i - \\hat{y}_i\\) as the ith residual. Later, I will discuss rescaling by, for example, \\(s\\), to get a standardized residual. Role of residuals: If the model formulation is correct, then residuals should be approximately equal to random errors or “white noise.” Method of attack: Look for patterns in the residuals. Use this information to improve the model specification. Overhead B. Using residuals to select explanatory variables Residual analysis can help identify additional explanatory variables that may be used to improve the formulation of the model. If the model is correct, then residuals should resemble random errors and contain no discernible patterns. Thus, when comparing residuals to explanatory variables, we do not expect any relationships. If we do detect a relationship, then this suggests the need to control for this additional variable. Overhead C. Detecting relationships between residuals and explanatory variables Calculate summary statistics and display the distribution of residuals to identify outliers. Calculate the correlation between the residuals and additional explanatory variables to search for linear relationships. Create scatter plots between the residuals and additional explanatory variables to search for nonlinear relationships. 4.3.2 Exercise. Residual analysis and risk manager survey Assignment Text This exercise examines data, pre-loaded in the dataframe survey, from a survey on the cost effectiveness of risk management practices. Risk management practices are activities undertaken by a firm to minimize the potential cost of future losses, such as the event of a fire in a warehouse or an accident that injures employees. This exercise develops a model that can be used to make statements about cost of managing risks. A measure of risk management cost effectiveness, logcost, is the outcome variable. This variable is defined as total property and casualty premiums and uninsured losses as a proportion of total assets, in logarithmic units. It is a proxy for annual expenditures associated with insurable events, standardized by company size. Explanatory variables include logsize, the logarithm of total firm assets, and indcost, a measure of the firm’s industry risk. Instructions Fit and summarize a MLR model using logcost as the outcome variable and logsize and indcost as explanatory variables. Plot residuals of the fitted model versus indcost and superimpose a locally fitted line using the R function lowess(). Fit and summarize a MLR model of logcost on logsize, indcost and a squared version of indcost. Plot residuals of the fitted model versus `indcost’ and superimpose a locally fitted line using lowess(). Hint You can access model residuals using mlr.survey1$residuals or mlr.survey1($residuals) Pre-exercise code # Pre-exercise code survey &lt;- read.csv(&quot;CSVData\\\\Risk_survey.csv&quot;, header=TRUE) #survey &lt;- read.csv(&quot;https://assets.datacamp.com/production/repositories/2610/datasets/dc1c5bce43ef076aa77169a242118e2e58d01f82/Risk_survey.csv&quot;, header=TRUE) survey$logcost &lt;- log(survey$firmcost) #str(survey) Sample_code `@sample_code` # Regress `logcost` on `logsize` and `indcost` mlr.survey1 &lt;- lm(logcost ~ logsize + indcost, data = survey) summary(___) # Plot residuals of the fitted model versus `indcost` and superimpose a locally fitted line using the function [lowess()] plot(survey$indcost, ___) lines(lowess(survey$indcost, ___)) # Regress `logcost` on `logsize` and `indcost` and `indcost` squared mlr.survey2 &lt;- lm(___ ~ logsize + poly(indcost,2), data = survey) summary(___) # Plot residuals of this fitted model and superimpose a locally fitted line using the function [lowess()] plot(survey$indcost, ___) lines(lowess(survey$indcost, ___)) Solution # Solution mlr.survey1 &lt;- lm(logcost ~ logsize + indcost, data = survey) summary(mlr.survey1) plot(survey$indcost, mlr.survey1$residuals) lines(lowess(survey$indcost,mlr.survey1$residuals)) mlr.survey2 &lt;- lm(logcost ~ logsize + poly(indcost,2), data = survey) summary(mlr.survey2) plot(survey$indcost, mlr.survey2$residuals) lines(lowess(survey$indcost,mlr.survey2$residuals)) Submission correctness test (SCT) success_msg(“Excellent! In this exercise, you examined residuals from a preliminary model fit and detected a mild quadratic pattern in a variable. This suggested entering the squared term of that variable into the model specification. The refit of this new model suggests that the squared term has important explanatory information. The squared term is a nonlinear alternative that is not available in many automatic variable selection procedures.”) 4.3.3 Exercise. Added variable plot and refrigerator prices Assignment Text What characteristics of a refrigerator are important in determining its price (price)? We consider here several characteristics of a refrigerator, including the size of the refrigerator in cubic feet (rsize), the size of the freezer compartment in cubic feet (fsize), the average amount of money spent per year to operate the refrigerator (ecost, for energy cost), the number of shelves in the refrigerator and freezer doors (shelves), and the number of features (features). The features variable includes shelves for cans, see-through crispers, ice makers, egg racks and so on. Both consumers and manufacturers are interested in models of refrigerator prices. Other things equal, consumers generally prefer larger refrigerators with lower energy costs that have more features. Due to forces of supply and demand, we would expect consumers to pay more for these refrigerators. A larger refrigerator with lower energy costs that has more features at the similar price is considered a bargain to the consumer. How much extra would the consumer be willing to pay for this additional space? A model of prices for refrigerators on the market provides some insight to this question. To this end, we analyze data from n = 37 refrigerators. Instructions # Pre-exercise code Refrig &lt;- read.table(&quot;CSVData\\\\Refrig.csv&quot;, header = TRUE, sep = &quot;,&quot;) summary(Refrig) Refrig1 &lt;- Refrig[c(&quot;price&quot;, &quot;ecost&quot;, &quot;rsize&quot;, &quot;fsize&quot;, &quot;shelves&quot;, &quot;s_sq_ft&quot;, &quot;features&quot;)] round(cor(Refrig1), digits = 3) refrig_mlr1 &lt;- lm(price ~ rsize + fsize + shelves + features, data = Refrig) summary(refrig_mlr1) Refrig$residuals1 &lt;- residuals(refrig_mlr1) refrig_mlr2 &lt;- lm(ecost ~ rsize + fsize + shelves + features, data = Refrig) summary(refrig_mlr2) Refrig$residuals2 &lt;- residuals(refrig_mlr2) plot(Refrig$residuals2, Refrig$residuals1) library(Rcmdr) refrig_mlr3 &lt;- lm(price ~ rsize + fsize + shelves + features + ecost, data = Refrig) avPlots(refrig_mlr3, terms = &quot;ecost&quot;) 4.4 Unusual observations 4.4.1 Video (Exercise). Unusual observations 4.4.1.1 Learning Objectives In this module, you learn how to: Compare and contrast three alternative definitions of a standardized residual Evaluate three alternative options for dealing with outliers Assess the impact of a high leverage observation Evaluate options for dealing with high leverage observations Describe the notion of influence and Cook’s Distance for quantifying influence 4.4.1.2 Video Overheads Overhead A. Unusual observations Regression coefficients can be expressed as (matrix) weighted averages of outcomes Averages, even weighted averages can be strongly influenced by unusual observations Observations may be unusual in the y direction or in the X space For unusual in the y direction, we use a residual \\(e = y - \\hat{y}\\) By subtracting the fitted value \\(\\hat{y}\\), we look to the y distance from the regression plane In this way, we “control” for values of explanatory variables Overhead B. Standardized residuals We standardize residuals so that we can focus on relationships of interest and achieve carry-over of experience from one data set to another. Three commonly used definitions of standardize residuals are: \\[ \\text{(a) }\\frac{e_i}{s}, \\ \\ \\ \\text{ (b) }\\frac{e_i}{s\\sqrt{1-h_{ii}}}, \\ \\ \\ \\text{(c)}\\frac{e_i}{s_{(i)}\\sqrt{1-h_{ii}}}. \\] First choice is simple Second choice, from theory, \\(\\mathrm{Var}(e_i)=\\sigma ^{2}(1-h_{ii}).\\) Here, \\(h_{ii}\\) is the \\(i\\)th leverage (defined later). Third choice is termed “studentized residuals”. Idea: numerator is independent of the denominator. Overhead C. Outlier - an unusal standardized residual An outlier is an observation that is not well fit by the model; these are observations where the residual is unusually large. Unusual means what? Many packages mark a point if the |standardized residual| &gt; 2. Options for handling outliers Ignore them in the analysis but be sure to discuss their effects. Delete them from the data set (but be sure to discuss their effects). Create a binary variable to indicator their presence. (This will increase your \\(R^2\\)!) Overhead D. High leverage points A high leverage point is an observation that is “far away” in the \\(x\\)-space from others. One can get a feel for high leverage observations by looking a summary statistics (mins, maxs) for each explanatory variable. Options for dealing with high leverage points are comparable to outliers, we can ignore their effects, delete them, or mark them with a binary indicator variable. Overhead E. High leverage point graph library(cluster) #library(MASS) par(mar=c(3.2,5.4,.2,.2)) plot(1,5,type=&quot;p&quot;,pch=19,cex=1.5,xlab=&quot;&quot;,ylab=&quot;&quot;,cex.lab=1.5,xaxt=&quot;n&quot;,yaxt=&quot;n&quot;,xlim=c(-3,5),ylim=c(-12,12)) mtext(expression(x[2]), side=1,line=2, cex=2.0) mtext(expression(x[1]), side=2, line=2, las=2, cex=2.0) arrows(1.5,5,4,5,code=1,lwd=2,angle=15,length=0.25) xycov&lt;-matrix(c(2, -5,-5, 20),nrow=2,ncol=2) xyloc&lt;-matrix(c(0, 0),nrow=1,ncol=2) polygon(ellipsoidPoints(xycov, d2 = 2, loc=xyloc),col=&quot;black&quot;) Overhead F. Leverage Using matrix algebra, one can express the ith fitted value as a linear combination of observations \\[ \\hat{y}_{i} = h_{i1} y_{1} + \\cdots +h_{ii}y_{i}+\\cdots+h_{in}y_{n}. \\] The term \\(h_{ii}\\) is known as the ith leverage The larger the value of \\(h_{ii}\\), the greater the effect of the ith observation \\(y_i\\) on the ith fitted value \\(\\hat{y}_i\\). Statistical routines have values of the leverage coded, so computing this quantity. The key thing to know is that \\(h_{ii}\\) is based solely on the explanatory variables. If you change the \\(y\\) values, the leverage does not change. As a commonly used rule of thumb, a leverage is deemed to be “unusual” if its value exceeds three times the average (= number of regression coefficients divided by the number of observations.) 4.4.2 Exercise. Outlier example In chapter 2, we consider a fictitious data set of 19 “base” points plus three different types of unusual points. In this exercise, we consider the effect of one unusal point, “C”, this both an outlier (unusual in the “y” direction) and a high leverage point (usual in the x-space). The data have been pre-loaded in the dataframe outlrC. Instructions Fit a basic linear regression model of y on x and store the result in an object. Use the function rstandard() to extract the standardized residuals from the fitted regression model object and summarize them. Use the function hatvalues() to extract the leverages from the model fitted and summarize them. Plot the standardized residuals versus the leverages to see the relationship between these two measures that calibrate how unusual an observation is. Hint Pre-exercise code # Pre-exercise code outlr &lt;- read.csv(&quot;CSVData\\\\Outlier.csv&quot;, header = TRUE) #outlr &lt;- read.csv(&quot;https://assets.datacamp.com/production/repositories/2610/datasets/7a38912e544c31fc6f5fca12b9a2eb645f2bcd32/Outlier.csv&quot;, header = TRUE) outlrC &lt;- outlr[-c(20,21),c(&quot;x&quot;,&quot;y&quot;)] Sample)code `@sample_code` outlrC &lt;- outlr[-c(20,21),c(&quot;x&quot;,&quot;y&quot;)] # Fit a basic linear regression model of `y` on `x` and store the result in an object. model_outlrC &lt;- lm(y ~ x, data = outlrC) # Extract the standardized residuals from the fitted regression model object and summarize them. ri &lt;- rstandard(model_outlrC) summary(ri) # Extract the leverages from the model fitted and summarize them. hii &lt;- hatvalues(model_outlrC) summary(hii) # Plot the standardized residuals versus the leverages plot(hii,ri) Solution # solution plot(outlrC) model_outlrC &lt;- lm(y ~ x, data = outlrC) ri &lt;- rstandard(model_outlrC) summary(ri) hii &lt;- hatvalues(model_outlrC) summary(hii) plot(hii,ri) Submission correctness test (SCT) success_msg(“Excellent! With only two variables, we could argue graphically that observations were unusual. In this exercise, we showed how statistics could also be used to identify usual observations. Although not really necessary in basic linear regression, the main advantage of the statistics is that they work readily in a multivariate setting.”) 4.4.3 Exercise. High leverage and risk manager survey Assignment Text In a prior exercise, we fit a regression model of logcost on logsize, indcost and a squared version of indcost. This model is summarized in the object mlr_survey2. In this exercise, we examine the robustness of the model to unusual observations. Instructions Use the R functions rstandard() and hatvalues() to extract the standardized residuals and leverages from the model fitted. Summarize the distributions graphically. You will see that there are two observations where the leverages are high, numbers 10 and 16. On looking at the dataset, these turn out to be observations in a high risk industry. Create a histogram of the variable indcost to corroborate this. Re-run the regression omitting observations 10 and 16. Summarize this regression and the regression in the object mlr_survey2, noting differences in the coefficients. Hint Pre-exercise code # Pre-exercise code survey &lt;- read.csv(&quot;CSVData\\\\Risk_survey.csv&quot;, header=TRUE) #survey &lt;- read.csv(&quot;https://assets.datacamp.com/production/repositories/2610/datasets/dc1c5bce43ef076aa77169a242118e2e58d01f82/Risk_survey.csv&quot;, header=TRUE) survey$logcost &lt;- log(survey$firmcost) mlr.survey2 &lt;- lm(logcost ~ logsize + poly(indcost,2), data = survey) Sample_code `@sample_code` mlr.survey2 &lt;- lm(logcost ~ logsize + poly(indcost,2), data = survey) # Extract the standardized residuals and leverages from the model fitted. Summarize the distributions graphically. ri &lt;- ___(mlr.survey2) hii &lt;- ___(mlr.survey2) par(mfrow=c(1, 2)) hist(ri, nclass=16, main=&quot;&quot;, xlab=&quot;Standardized Residuals&quot;) hist(hii, nclass=16, main=&quot;&quot;, xlab=&quot;Leverages&quot;) # Create a histogram of the variable `indcost` par(mfrow=c(1, 1)) hist(___, nclass=16) # Re-run the regression omitting observations 10 and 16. Summarize this regression and the regression in the object `mlr_survey2`, noting differences in the coefficients. mlr.survey3 &lt;- lm(___ ~ logsize + poly(indcost,2), data = survey, subset =-c(10,16)) summary(mlr.survey2) summary(mlr.survey3) Solution # Solution #summary(mlr.survey2) ri &lt;- rstandard(mlr.survey2) hii &lt;- hatvalues(mlr.survey2) par(mfrow=c(1, 2)) hist(ri, nclass=16, main=&quot;&quot;, xlab=&quot;Standardized Residuals&quot;) hist(hii, nclass=16, main=&quot;&quot;, xlab=&quot;Leverages&quot;) par(mfrow=c(1, 1)) hist(survey$indcost, nclass=16) mlr.survey3 &lt;- lm(logcost ~ logsize + poly(indcost,2), data = survey, subset =-c(10,16)) summary(mlr.survey2) summary(mlr.survey3) Submission correctness test (SCT) success_msg(“Excellent! You will have noted that after removing these two influential observations from a high risk industry, the variable associated with the indcost squared became less statistically significant. This illustrates a general phenomena; sometimes, the ‘signicance’ of a variable may actually due to a few unusual observations, not the entire variable.”) 4.5 Collinearity 4.5.1 Video (Exercise). Collinearity 4.5.1.1 Learning Objectives In this module, you learn how to: Define collinearity and describe its potential impact on regression inference Define a variance inflation factor and describe its effect on a regression coefficients standard error Describe rules of thumb for assessing collinearity and options for model reformulation in the presence of severe collinearity Compare and contrast effects of leverage and collinearity 4.5.1.2 Video Overheads Overhead A. Collinearity Collinearity, or multicollinearity, occurs when one explanatory variable is, or nearly is, a linear combination of the other explanatory variables. Useful to think of the explanatory variables as being highly correlated with one another. Collinearity neither precludes us from getting good fits nor from making predictions of new observations. Estimates of error variances and, therefore, tests of model adequacy, are still reliable. In cases of serious collinearity, standard errors of individual regression coefficients can be large. With large standard errors, individual regression coefficients may not be meaningful. Because a large standard error means that the corresponding t-ratio is small, it is difficult to detect the importance of a variable. Overhead B. Quantifying collinearity A common way to quantify collinearity is through the variance inflation factor (VIF). Suppose that the set of explanatory variables is labeled \\(x_{1},x_{2},\\dots,x_{k}\\). Run the regression using \\(x_{j}\\) as the “outcome” and the other \\(x\\)’s as the explanatory variables. Denote the coefficient of determination from this regression by \\(R_j^2\\). Define the variance inflation factor \\[ VIF_{j}=\\frac{1}{1-R_{j}^{2}},\\ \\ \\ \\text{ for } j = 1,2,\\ldots, k. \\] Overhead C. Options for handling collinearity Rule of thumb: When \\(VIF_{j}\\) exceeds 10 (which is equivalent to \\(R_{j}^{2}&gt;90\\%\\)), we say that severe collinearity exists. This may signal is a need for action. Recode the variables by “centering” - that is, subtract the mean and divide by the standard deviation. Ignore the collinearity in the analysis but comment on it in the interpretation. Probably the most common approach. Replace one or more variables by auxiliary variables or transformed versions. Remove one or more variables. Easy. Which One? is hard. Use interpretation. Which variable(s) do you feel most comfortable with? Use automatic variable selection procedures to suggest a model. 4.5.2 Exercise. Collinearity and term life Assignment Text We have seen that adding an explanatory variable \\(x^2\\) to a model is sometimes helpful even though it is perfectly related to \\(x\\) (such as through the function \\(f(x)=x^2\\)). But, for some data sets, higher order polynomials and interactions can be approximately linearly related (depending on the range of the data). This exercise returns to our term life data set Term1 (preloaded) and demonstrates that collinearity can be severe when introducing interaction terms. Instructions Fit a MLR model of logface on explantory variables education, numhh and logincome Use the function vif() from the car package (preloaded) to calculate variance inflation factors. Fit and summarize a MLR model of logface on explantory variables education , numhh and logincome with an interaction between numhh and logincome, then extract variance inflation factors. Hint If the car package is not available to you, then you could calculate vifs using the [lm()] function, treating each variable separately. For example 1/(1-summary(lm(education ~ numhh + logincome, data = Term1))$r.squared) gives the education vif. Pre-exercise code # Pre-exercise code Term &lt;- read.csv(&quot;CSVData\\\\term_life.csv&quot;, header = TRUE) #Term &lt;- read.csv(&quot;https://assets.datacamp.com/production/repositories/2610/datasets/efc64bc2d78cf6b48ad2c3f5e31800cb773de261/term_life.csv&quot;, header = TRUE) Term1 &lt;- subset(Term, subset = face &gt; 0) #str(Term1) *Sample_code** `@sample_code` # Fit a MLR model of `logface` on explantory variables `education`, `numhh` and `logincome` Term_mlr &lt;- lm(logface ~ education + numhh + logincome, data = Term1) # Calculate the variance inflation factors. car::vif(Term_mlr) # Fit and summarize a MLR model of `logface` on explantory variables `education` , `numhh` and `logincome` with an interaction between `numhh` and `logincome`, then extract variance inflation factors. Term_mlr1 &lt;- lm(logface ~ education + numhh*logincome , data = Term1) summary(Term_mlr1) car::vif(Term_mlr1) Solution # Solution Term_mlr &lt;- lm(logface ~ education + numhh + logincome, data = Term1) car::vif(Term_mlr) Term_mlr1 &lt;- lm(logface ~ education + numhh*logincome , data = Term1) summary(Term_mlr1) car::vif(Term_mlr1) Submission correctness test (SCT) success_msg(“Excellent! This exercise underscores that colinearity among explanatory variables can be induced when introducing higher order terms such as interactions. Note that in the interaction model the variable ‘numhh’ does not appear to be statistically signficant effect. This is one of the big dangers of collinearity - it can mask important effects.”) 4.6 Selection criteria 4.6.1 Video (Exercise). Selection criteria 4.6.1.1 Learning Objectives In this module, you learn how to: Summarize a regression fit using alternative goodness of fit measures Validate a model using in-sample and out-of-sample data to mitigate issues of data-snooping Compare and contrast SSPE and PRESS statistics for model validation 4.6.1.2 Video Overheads Overhead A. Goodness of fit Criteria that measure the proximity of the fitted model and realized data are known as goodness of fit statistics. Basic examples include: the coefficient of determination \\((R^{2})\\), an adjusted version \\((R_{a}^{2})\\), the size of the typical error \\((s)\\), and \\(t\\)-ratios for each regression coefficient. Overhead B. Goodness of fit and information criteria A general measure is Akaike’s Information Criterion, defined as \\[ AIC = -2 \\times (fitted~log~likelihood) + 2 \\times (number~of~parameters) \\] For model comparison, the smaller the \\(AIC,\\) the better is the fit. This measures balances the fit (in the first part) with a penalty for complexity (in the second part) It is a general measure - for linear regression, it reduces to \\[ AIC = n \\ln (s^2) + n \\ln (2 \\pi) +n +k + 3 . \\] So, selecting a model to minimize \\(s\\) or \\(s^2\\) is equivalent to model selection based on minimizing \\(AIC\\) (same k). Overhead C. Out of sample validation When you choose a model to minimize \\(s\\) or \\(AIC\\), it is based on how well the model fits the data at hand, or the model development, or training, data As we have seen, this approach is susceptible to overfitting. A better approach is to validate the model on a model validation, or test data set, held out for this purpose. Overhead D. Out of sample validation procedure Using the model development subsample, fit a candidate model. Using the Step (ii) model and the explanatory variables from the validation subsample, “predict” the dependent variables in the validation subsample, \\(\\hat{y}_i\\), where \\(i=n_{1}+1,...,n_{1}+n_{2}\\). Calc the *sum of absolute prediction errors** \\[SAPE=\\sum_{i=n_{1}+1}^{n_{1}+n_{2}} |y_{i}-\\hat{y}_{i}| . \\] Repeat Steps (i) through (iii) for each candidate model. Choose the model with the smallest SAPE. Overhead E. Cross - validation With out-of-sample validation, the statistic depends on a random split between in-sample and out-of-sample data (a problem for data sets that are not large) Alternatively, one may use cross-validation Use a random mechanism to split the data into k subsets, (e.g., 5-10) Use the first k-1 subsamples to estimate model parameters. Then, “predict” the outcomes for the kth subsample and use SAE to summarize the fit Repeat this by holding out each of the k sub-samples, summarizing with a cumulative SAE. Repeat these steps for several candidate models. Choose the model with the lowest cumulative SAE statistic. 4.6.2 Exercise. Cross-validation and term life Assignment Text Here is some sample code to give you a better feel for cross-validation. The first part of the randomly re-orders (“shuffles”) the data. It also identifies explanatory variables explvars. The function starts by pulling out only the needed data into cvdata. Then, for each subsample, a model is fit based on all the data except for the subsample, in train_mlr with the subsample in test. This is repeated for each subsample, then results are summarized. # Randomly re-order data - &quot;shuffle it&quot; n &lt;- nrow(Term1) set.seed(12347) shuffled_Term1 &lt;- Term1[sample(n), ] explvars &lt;- c(&quot;education&quot;, &quot;numhh&quot;, &quot;logincome&quot;) ## Cross - Validation crossvalfct &lt;- function(explvars){ cvdata &lt;- shuffled_Term1[, c(&quot;logface&quot;, explvars)] crossval &lt;- 0 k &lt;- 5 for (i in 1:k) { indices &lt;- (((i-1) * round((1/k)*nrow(cvdata))) + 1):((i*round((1/k) * nrow(cvdata)))) # Exclude them from the train set train_mlr &lt;- lm(logface ~ ., data = cvdata[-indices,]) # Include them in the test set test &lt;- data.frame(cvdata[indices, explvars]) names(test) &lt;- explvars predict_test &lt;- exp(predict(train_mlr, test)) # Compare predicted to held-out and summarize predict_err &lt;- exp(cvdata[indices, &quot;logface&quot;]) - predict_test crossval &lt;- crossval + sum(abs(predict_err)) } crossval/1000 } crossvalfct(explvars) Instructions Calculate the cross-validation statistic using only logarithmic income, logincome. Calculate the cross-validation statistic using logincome, education and numhh. Calculate the cross-validation statistic using logincome, education, numhh and marstat. The best model has the lowest cross-validation statistic. Hint The function [sample()] is for taking random samples. We use it without replacement so it results in a re-ordering of data. Pre-exercise code # Pre-exercise code Term &lt;- read.csv(&quot;CSVData\\\\term_life.csv&quot;, header = TRUE) #Term &lt;- read.csv(&quot;https://assets.datacamp.com/production/repositories/2610/datasets/efc64bc2d78cf6b48ad2c3f5e31800cb773de261/term_life.csv&quot;, header = TRUE) Term1 &lt;- subset(Term, subset = face &gt; 0) Term1$marstat &lt;- as.factor(Term1$marstat) crossvalfct &lt;- function(explvars){ cvdata &lt;- shuffled_Term1[, c(&quot;logface&quot;, explvars)] crossval &lt;- 0 k &lt;- 5 for (i in 1:k) { indices &lt;- (((i-1) * round((1/k)*nrow(cvdata))) + 1):((i*round((1/k) * nrow(cvdata)))) # Exclude them from the train set train_mlr &lt;- lm(logface ~ ., data = cvdata[-indices,]) # Include them in the test set test &lt;- data.frame(cvdata[indices, explvars]) names(test) &lt;- explvars predict_test &lt;- exp(predict(train_mlr, test)) # Compare predicted to held-out and summarize predict_err &lt;- exp(cvdata[indices, &quot;logface&quot;]) - predict_test crossval &lt;- crossval + sum(abs(predict_err)) } crossval/1000000 } Sample_code `@sample_code` # Calculate the cross-validation statistic using only logarithmic income, `logincome`. explvars &lt;- c(&quot;logincome&quot;) crossvalfct(explvars) # Calculate the cross-validation statistic using `logincome`, `education` and `numhh`. explvars &lt;- c(&quot;education&quot;, &quot;numhh&quot;, &quot;logincome&quot;) crossvalfct(___) # Calculate the cross-validation statistic using `logincome`, `education`, `numhh` and `marstat`. explvars &lt;- c(___) crossvalfct(explvars) Solution # Solution # Randomly re-order data - &quot;shuffle it&quot; n &lt;- nrow(Term1) set.seed(12347) shuffled_Term1 &lt;- Term1[sample(n), ] ## Cross - Validation explvars &lt;- c(&quot;logincome&quot;) crossvalfct(explvars) explvars &lt;- c(&quot;education&quot;, &quot;numhh&quot;, &quot;logincome&quot;) crossvalfct(explvars) explvars &lt;- c(&quot;education&quot;, &quot;numhh&quot;, &quot;logincome&quot;, &quot;marstat&quot;) crossvalfct(explvars) Submission correctness test (SCT) success_msg(“Excellent! This exercises demonstrates the use of cross-validation, a very important technique in model selection. The exercise builds the procedure from the ground up so that you can see all the steps involved. Further, it illustrates how you can develop your own functions to automate procedures and save steps.”) "],
["interpreting-regression-results.html", "Chapter 5 Interpreting Regression Results 5.1 Case study: MEPS health expenditures 5.2 What the modeling procedure tells us 5.3 The importance of variable selection", " Chapter 5 Interpreting Regression Results Chapter description A case study, on determining an individual’s characteristics that influence its health expenditures, illustrates the regression modeling process from start to finish. Subsequently, the chapter summarizes what we learn from the modeling process, underscoring the importance of variable selection. 5.1 Case study: MEPS health expenditures 5.1.1 Video (Exercise). Case study: MEPS health expenditures 5.1.1.1 Learning Objectives In this module, you learn how to: 5.1.1.2 Video Overheads Overhead A. MEPS health expenditures This exercise considers data from the Medical Expenditure Panel Survey (MEPS), conducted by the U.S. Agency of Health Research and Quality. MEPS is a probability survey that provides nationally representative estimates of health care use, expenditures, sources of payment, and insurance coverage for the U.S. civilian population. This survey collects detailed information on individuals of each medical care episode by type of services including physician office visits, hospital emergency room visits, hospital outpatient visits, hospital inpatient stays, all other medical provider visits, and use of prescribed medicines. This detailed information allows one to develop models of health care utilization to predict future expenditures. You can learn more about MEPS at http://www.meps.ahrq.gov/mepsweb/. We consider MEPS data from the panels 7 and 8 of 2003 that consists of 18,735 individuals between ages 18 and 65. From this sample, we took a random sample of 2,000 individuals that appear in the file HealthExpend. From this sample, there are 1,352 that had positive outpatient expenditures. Our dependent variable is the amount of expenditures for outpatient visits, expendop. For MEPS, outpatient events include hospital outpatient department visits, office-based provider visits and emergency room visits excluding dental services. (Dental services, compared to other types of health care services, are more predictable and occur in a more regular basis.) Hospital stays with the same date of admission and discharge, known as “zero-night stays,” were included in outpatient counts and expenditures. (Payments associated with emergency room visits that immediately preceded an inpatient stay were included in the inpatient expenditures. Prescribed medicines that can be linked to hospital admissions were included in inpatient expenditures, not in outpatient utilization.) Overhead B. Overhead MEPS health expenditures Data from the Medical Expenditure Panel Survey (MEPS), conducted by the U.S. Agency of Health Research and Quality (AHRQ). A probability survey that provides nationally representative estimates of health care use, expenditures, sources of payment, and insurance coverage for the U.S. civilian population. Collects detailed information on individuals of each medical care episode by type of services including physician office visits, hospital emergency room visits, hospital outpatient visits, hospital inpatient stays, all other medical provider visits, and use of prescribed medicines. This detailed information allows one to develop models of health care utilization to predict future expenditures. We consider MEPS data from the first panel of 2003 and take a random sample of n = 2, 000 individuals between ages 18 and 65. Overhead C. Outcome variable Our dependent variable is expenditures for outpatient admissions. For MEPS, inpatient admissions include persons who were admitted to a hospital and stayed overnight. In contrast, outpatient events include hospital outpatient department visits, office-based provider visits and emergency room visits excluding dental services. Hospital stays with the same date of admission and discharge, known as “zero-night stays,” were included in outpatient counts and expenditures. Payments associated with emergency room visits that immediately preceded an inpatient stay were included in the inpatient expenditures. Prescribed medicines that can be linked to hospital admissions were included in inpatient expenditures, not in outpatient utilization. Overhead D. Explanatory variables 9 variables in the database. Here 13 most relevant. \\[ {\\small \\begin{array}{ll} expendop &amp; \\text{Amounts of expenditures for outpatient visits} \\\\ gender &amp; \\text{Indicate gender of patient (=1 if female, =0 if male)} \\\\ age &amp; \\text{Age in years between 18 and 65 }\\\\ race &amp; \\text{Race of patient described as Asian, Black, Native, White and other} \\\\ region &amp; \\text{Region of patient described as WEST, NORTHEAST, MIDWEST and SOUTH} \\\\ educ &amp; \\text{Level of education received described by words (LHIGHSC, HIGHSCH and COLLEGE)} \\\\ phstat &amp; \\text{Self-rated physical health status described as EXCE, VGOO, GOOD, FAIR and POOR} \\\\ mpoor &amp; \\text{Self-rated mental health (=1 if poor or fair, =0 if good to excellent mental health)} \\\\ anylimit &amp; \\text{Any activity limitation (=1 if any functional/activity limitation, =0 if otherwise)} \\\\ income &amp; \\text{Income compared to poverty line described as POOR, NPOOR, LINCOME, MINCOME and HINCOME} \\\\ insure &amp; \\text{Insurance coverage (=1 if covered by public/private health insurance in any month of 1996, =0 otherwise)} \\\\ usc &amp; \\text{1 if dissatisfied with one&#39;s usual source of care} \\\\ unemploy &amp; \\text{Employment status of patients} \\\\ managedcare &amp; \\text{1 if enrolled in an HMO or gatekeeper plan} \\\\ \\end{array}} \\] Overhead E. Case study outline The next series of exercises leads you through an analysis of the steps for understanding a complex data set. Because of the complexity of the data, in each step only a sample of procedures will be executed. The outline consists of: Summary statistics Splitting the data into training and testing portions with initial model fits Selecting variables to be included in the model 5.1.2 Exercise. Summarizing data Assignment Text With a complex dataset, you will probably want to take a look at the structure of the data. You are already familiar with taking a [summary()] of a dataframe which provides summary statistics for many variables. You will see that several variables in this dataframe are categorical, or factor, variables. We can use the table() function to summarize them. After getting a sense of the distributions of explanatory variables, we want to take a deeper dive into the distribution of the outcome variable, expendop. We will do this by comparing the histograms of the variable to that of its logarithmic version. To examine relationships of the outcome variable visually, we look to scatterplots for continuous variables (such as age) and boxplots for categorical variables (such as phstat). Instructions Examine the structure of the meps dataframe using the str() function. Also, get a [summary()] of the dataframe. Examine the distribution of the race variable using the table() function. Compare the expenditures distribution to its logarithmic version visually via histograms plotted next to another. par(mfrow = c(1, 2)) is used to organize the plots you create. Examine the distribution of logarithmic expenditures in terms of levels of phstat visually using the boxplot() function. Examine the relationship of age versus logarithmic expenditures using a scatter plot. Superimpose a local fitting line using the lines() and lowess() functions. Hint Pre-exercise code # Pre-exercise code meps &lt;- read.csv(&quot;CSVData\\\\HealthMeps.csv&quot;, header = TRUE) #meps &lt;- read.csv(&quot;https://assets.datacamp.com/production/repositories/2610/datasets/7b7dab6d0c528e4cd2f8d0e0fc7824a254429bf8/HealthMeps.csv&quot;, header = TRUE) Sample_code `@sample_code` # Examine the structure and get a summary of the `meps` dataframe str(___) summary(___) # Examine the distribution of the `race` variable table(___) # Compare the expenditures distribution to its logarithmic version visually par(mfrow = c(1, 2)) hist(___, main = &quot;&quot;, xlab = &quot;outpatient expenditures&quot;) hist(log(___), main = &quot;&quot;, xlab = &quot;log expenditures&quot;) # Examine the distribution of logarithmic expenditures in terms of levels of `phstat` par(mfrow = c(1, 1)) meps$logexpend &lt;- log(meps$expendop) boxplot(logexpend ~ ___, data = meps, main = &quot;boxplot of log expend&quot;) # Examine the relationship of age versus logarithmic expenditures. Superimpose a local fitting line. plot(___,___, xlab = &quot;age&quot;, ylab = &quot;log expend&quot;) lines(lowess(___, ___), col=&quot;red&quot;) Solution # Solution str(meps) summary(meps) table(meps$race) par(mfrow = c(1, 2)) hist(meps$expendop, main = &quot;&quot;, xlab = &quot;outpatient expenditures&quot;) hist(log(meps$expendop), main = &quot;&quot;, xlab = &quot;log expenditures&quot;) par(mfrow = c(1, 1)) meps$logexpend &lt;- log(meps$expendop) boxplot(logexpend ~ phstat, data = meps, main = &quot;boxplot of log expend&quot;) plot(meps$age,meps$logexpend, xlab = &quot;age&quot;, ylab = &quot;log expend&quot;) lines(lowess(meps$age, meps$logexpend), col=&quot;red&quot;) Submission correctness test (SCT) success_msg(“Excellent! Summarizing data, without reference to a model, is probably the most time-consuming part of any predictive modeling exercise. Summary statistics are also a key part of any report as they illustrate features of the data that are accessible to a broad audience.”) 5.1.3 Exercise. Fit a benchmark multiple linear regression model Assignment Text As part of the pre-processing for the model fitting, we will split the data into training and test subsamples. For this exercise, we use a 75/25 split although other choices are certainly suitable. Some analysts prefer to do this splitting before looking at the data. Another approach, adopted here, is that the final report typically contains summary statistcs of the entire data set and so it makes sense to do so when examining summary statistics. We start by fitting a benchmark model. It is common to use all available explanatory variables with the outcome on the original scale and so we use this as our benchmark model. This exercise shows that when you plot() a fitted linear regression model in R, the result provides four graphs that you have seen before. These can be useful for identifying an appropriate model. Instructions Randomly split the data into a training and a testing data sets. Use 75% for the training, 25% for the testing. Fit a full model using expendop as the outcome and all explanatory variables. Summarize the results of this model fitting. You can plot() the fitted model to view several diagnostic plots. These plots provide evidence that expenditures may not be the best scale for linear regression. Fit a full model using logexpend as the outcome and all explanatory variables and summarize the fit. Use the plot() function for evidence that this variable is more suited for linear regression methods than expenditures on the original scale. Hint A plot of a regression object such as plot(mlr) provides four diagnostic plots. These can be organized as a 2 by 2 array using par(mfrow = c(2, 2)). Pre-exercise code # Pre-exercise code meps &lt;- read.csv(&quot;CSVData\\\\HealthMeps.csv&quot;, header = TRUE) #meps &lt;- read.csv(&quot;https://assets.datacamp.com/production/repositories/2610/datasets/7b7dab6d0c528e4cd2f8d0e0fc7824a254429bf8/HealthMeps.csv&quot;, header = TRUE) meps$logexpend &lt;- log(meps$expendop) Sample_code `@sample_code` # Randomly split the data into a training and a testing data sets. Use 75\\% for the training, 25\\% for the testing. n &lt;- nrow(meps) set.seed(12347) shuffled_meps &lt;- meps[sample(n), ] train_indices &lt;- 1:round(0.75 * n) train_meps &lt;- shuffled_meps[train_indices, ] test_indices &lt;- (round(0.25 * n) + 1):n test_meps &lt;- shuffled_meps[test_indices, ] # Fit a full model using `expendop` as the outcome and all explanatory variables. Summarize the results of this model fitting. meps_mlr1 &lt;- lm(___ ~ gender + age + race + region + educ + phstat + mpoor + anylimit + income + insure + usc + unemploy + managedcare, data = ___) summary(meps_mlr1) # Provide diagnostic plots of the fitted model. par(mfrow = c(2, 2)) plot(___) # Fit a full model using `logexpend` as the outcome and all explanatory variables. Summarize the fit and examine diagnostic plots of the fitted model. meps_mlr2 &lt;- lm(___ ~ gender + age + race + region + educ + phstat + mpoor + anylimit + income + insure + usc + unemploy + managedcare, data = ___) summary(meps_mlr2) plot(meps_mlr2) Solution # Solution # Split the sample into a `training` and `test` data n &lt;- nrow(meps) set.seed(12347) shuffled_meps &lt;- meps[sample(n), ] train_indices &lt;- 1:round(0.75 * n) train_meps &lt;- shuffled_meps[train_indices, ] test_indices &lt;- (round(0.25 * n) + 1):n test_meps &lt;- shuffled_meps[test_indices, ] meps_mlr1 &lt;- lm(expendop ~ gender + age + race + region + educ + phstat + mpoor + anylimit + income + insure + usc + unemploy + managedcare, data = train_meps) summary(meps_mlr1) par(mfrow = c(2, 2)) plot(meps_mlr1) meps_mlr2 &lt;- lm(logexpend ~ gender + age + race + region + educ + phstat + mpoor + anylimit + income + insure + usc + unemploy + managedcare, data = train_meps) summary(meps_mlr2) plot(meps_mlr2) Submission correctness test (SCT) success_msg(“Excellent! You may have compared the four diagnostic graphs from the MLR model fit of ‘expend’ to those created using the same procedure but with logarithmic expenditures as the outcome. This provides another piece of evidence that log expenditures are more suitable for regression modeling. Using logarithmic outcomes is a common feature of actuarial applications but can be difficult to diagnose and interpret without practice.”) 5.1.4 Exercise. Variable selection Assignment Text Modeling building can be approached using a “ground-up” strategy, where the analyst introduces a variable, examines residuls from a regression fit, and then seeks to understand the relationship between these residuals and other available variables so that these variables might be added to the model. Another approach is a “top-down” strategy where all available variables are entered into a model and unnecessary variables are pruned from the model. Both approaches are helpful when using data to specify models. This exercise illustrates the latter approach, using the [step()] function to help narrow our search for the best fitting model. Instructions From our prior work, the training dataframe train_meps has already been loaded in. A multiple linear regression model fit object meps_mlr2 is available that summarizes a fit of logexpend as the outcome variable using all 13 explanatory variables. Use the step() function function to drop unnecessary variables from the full fitted model summarized in the object meps_mlr2 and summarize this recommended model. As an alternative, use the explanatory variables in the recommended model and add the varibles phstat. Summarize the fit and note that statistical significance of the new variable. You have been reminded by your boss that use of the variable gender is unsuitable for actuarial pricing purposes. As an another alternative, drop gender from the recommended model (still keeping phstat). Note the statistical significance of the variable uscwith this fitted model. Hint Pre-exercise code # Pre-exercise code meps &lt;- read.csv(&quot;CSVData\\\\HealthMeps.csv&quot;, header = TRUE) #meps &lt;- read.csv(&quot;https://assets.datacamp.com/production/repositories/2610/datasets/7b7dab6d0c528e4cd2f8d0e0fc7824a254429bf8/HealthMeps.csv&quot;, header = TRUE) meps$logexpend &lt;- log(meps$expendop) # Split the sample into a `training` and `test` data n &lt;- nrow(meps) set.seed(12347) shuffled_meps &lt;- meps[sample(n), ] train_indices &lt;- 1:round(0.75 * n) train_meps &lt;- shuffled_meps[train_indices, ] test_indices &lt;- (round(0.25 * n) + 1):n test_meps &lt;- shuffled_meps[test_indices, ] Sample_code `@sample_code` meps_mlr2 &lt;- lm(logexpend ~ gender + age + race + region + educ + phstat + mpoor + anylimit + income + insure + usc + unemploy + managedcare, data = train_meps) # Use the step() to drop unnecessary variables from the full fitted model summarized in the object `meps_mlr2` and summarize this recommended model. model_stepwise &lt;- step(meps_mlr2, data = ___, direction= &quot;both&quot;, k = log(nrow(X)), trace = 0) summary(model_stepwise) # As an alternative, use the explanatory variables in the recommended model and add the varibles `mpoor`. Summarize the fit and note that statistical significance of the new variable. meps_mlr4 &lt;- lm(___ ~ gender + age + phstat + anylimit + insure + ___, data = train_meps) summary(meps_mlr4) # You have been reminded by your boss that use of the variable `gender` is unsuitable for actuarial pricing purposes. As an another alternative, drop `gender` from the recommended model (still keeping `mpoor`). Note the statistical significance of the variable `usc`with this fitted model. meps_mlr5 &lt;- lm(logexpend ~ age + phstat + anylimit + insure + ___, data = train_meps) summary(___) Solution # Solution meps_mlr2 &lt;- lm(logexpend ~ gender + age + race + region + educ + phstat + mpoor + anylimit + income + insure + usc + unemploy + managedcare, data = train_meps) #library(Rcmdr) #temp &lt;- stepwise(meps_mlr2, direction = &#39;backward/forward&#39;) model_stepwise &lt;- step(meps_mlr2, data = X, direction= &quot;both&quot;, k = log(nrow(X)), trace = 0) summary(model_stepwise) meps_mlr3 &lt;- lm(logexpend ~ gender + age + phstat + anylimit + insure , data = train_meps) summary(meps_mlr3) meps_mlr4 &lt;- lm(logexpend ~ gender + age + phstat + anylimit + insure + mpoor, data = train_meps) summary(meps_mlr4) meps_mlr5 &lt;- lm(logexpend ~ age + phstat + anylimit + insure + mpoor, data = train_meps) summary(meps_mlr5) # par(mfrow = c(2, 2)) # plot(meps_mlr3) # # meps_mlr4 &lt;- lm(logexpend ~ gender + age + mpoor + anylimit + insure + usc + phstat, data = train_meps) # summary(meps_mlr4) # # # meps_mlr5 &lt;- lm(logexpend ~ age + anylimit + mpoor + insure + usc + phstat, data = train_meps) # summary(meps_mlr5) # anova(meps_mlr4, meps_mlr5) # # #boxplot(train_meps$logexpend ~ train_meps$phstat*train_meps$usc) Submission correctness test (SCT) success_msg(“Excellent! Sometimes variables may have good predictive power but are unacceptable for policy purposes - in insurance, ethnicity and sometimes sex are good examples. This implies that model interpretation can be just as important as the ability to predict.”) 5.1.5 Exercise. Model comparisons using cross-validation Assignment Text To compare alternative models, you decide to utilize cross-validation. For this exercise, you split the training sample into six subsamples of approximately equal size. In the sample code, the cross-validation procedure has been summarized into a function that you can call. The input to the function is a list of variables that you select as your model explanatory variables. With this function, you can readily test several candidate models. Instructions Run the cross validation (crossvalfct) function using the explanatory variables suggested by the stepwise function. Run the function again but adding the mpoor variable Run the function again but omitting the gender variable Note which model is suggested by the cross validation function. Hint The cross validation function of this is very similar to the one we did earlier. Different number of subsamples, different test/training data and a different outcome variable. Except for these minor changes, it is the same function that we worked with earlier. Pre-exercise code # Pre-exercise code meps &lt;- read.csv(&quot;CSVData\\\\HealthMeps.csv&quot;, header = TRUE) #meps &lt;- read.csv(&quot;https://assets.datacamp.com/production/repositories/2610/datasets/7b7dab6d0c528e4cd2f8d0e0fc7824a254429bf8/HealthMeps.csv&quot;, header = TRUE) meps$logexpend &lt;- log(meps$expendop) # Split the sample into a `training` and `test` data n &lt;- nrow(meps) set.seed(12347) shuffled_meps &lt;- meps[sample(n), ] train_indices &lt;- 1:round(0.75 * n) train_meps &lt;- shuffled_meps[train_indices, ] test_indices &lt;- (round(0.25 * n) + 1):n test_meps &lt;- shuffled_meps[test_indices, ] ## Cross - Validation crossvalfct &lt;- function(explvars){ cvdata &lt;- train_meps[, c(&quot;logexpend&quot;, explvars)] crossval &lt;- 0 for (i in 1:6) { indices &lt;- (((i-1) * round((1/6)*nrow(cvdata))) + 1):((i*round((1/6) * nrow(cvdata)))) # Exclude them from the train set train_mlr &lt;- lm(logexpend ~ ., data = cvdata[-indices,]) # Include them in the test set test &lt;- data.frame(cvdata[indices, explvars]) names(test) &lt;- explvars predict_test &lt;- exp(predict(train_mlr, test)) # Compare predicted to held-out and summarize predict_err &lt;- exp(cvdata[indices, &quot;logexpend&quot;]) - predict_test crossval &lt;- crossval + sum(abs(predict_err)) } crossval/1000000 } Sample_code `@sample_code` # Run the cross validation (`crossvalfct`) function using the explanatory variables suggested by the stepwise function. explvars &lt;- c(&quot;gender&quot;, &quot;age&quot;, &quot;phstat&quot;, &quot;anylimit&quot;, &quot;insure&quot;) crossvalfct(explvars) # Run the function again but adding the `mpoor` variable explvars &lt;- c(___) crossvalfct(explvars) # Run the function again but omitting the `gender` variable explvars &lt;- c( ___) crossvalfct(explvars) Solution #Solution explvars &lt;- c(&quot;gender&quot;, &quot;age&quot;, &quot;phstat&quot;, &quot;anylimit&quot;, &quot;insure&quot;) crossvalfct(explvars) explvars &lt;- c(&quot;gender&quot;, &quot;age&quot;, &quot;phstat&quot;, &quot;anylimit&quot;, &quot;insure&quot;, &quot;mpoor&quot;) crossvalfct(explvars) explvars &lt;- c( &quot;age&quot;, &quot;phstat&quot;, &quot;anylimit&quot;, &quot;insure&quot;, &quot;mpoor&quot;) crossvalfct(explvars) Submission correctness test (SCT) success_msg(“Excellent! Cross-validation has become an essential piece of the data analysts toolkit. Good that you now have additional experience with it.”) 5.1.6 Exercise. Out of sample validation Assignment Text From our prior work, the training train_meps and test test_meps dataframes have already been loaded in. We think our best model is based on logarithmic expenditures as the outcome and the following explanatory variables: explvars3 &lt;- c(&quot;gender&quot;, &quot;age&quot;, &quot;phstat&quot;, &quot;anylimit&quot;, &quot;insure&quot;, &quot;mpoor&quot;) We will compare this to a benchmark model that is based on expenditures as the outcome and all 13 explanatory variables explvars4 &lt;- c(explvars3, &quot;race&quot;, &quot;income&quot;, &quot;region&quot;, &quot;educ&quot;, &quot;unemploy&quot;, &quot;managedcare&quot;, &quot;usc&quot;) The comparisons will be based on expenditures in dollars using the held-out validation sample. Instructions Use the training sample to fit a linear model with logexpend and explanatory variables listed in explvars3 Predict expenditures (not logged) for the test data and summarize the fit using the sum of absolute prediction errors. Use the training sample to fit a benchmark linear model with expendop and explanatory variables listed in explvars4 Predict expenditures for the test data and summarize the fit for the benchmark model using the sum of absolute prediction errors. Compare the predictions of the models graphically. Hint Pre-exercise code # Pre-exercise code meps &lt;- read.csv(&quot;CSVData\\\\HealthMeps.csv&quot;, header = TRUE) #meps &lt;- read.csv(&quot;https://assets.datacamp.com/production/repositories/2610/datasets/7b7dab6d0c528e4cd2f8d0e0fc7824a254429bf8/HealthMeps.csv&quot;, header = TRUE) meps$logexpend &lt;- log(meps$expendop) # Split the sample into a `training` and `test` data n &lt;- nrow(meps) set.seed(12347) shuffled_meps &lt;- meps[sample(n), ] train_indices &lt;- 1:round(0.75 * n) train_meps &lt;- shuffled_meps[train_indices, ] test_indices &lt;- (round(0.25 * n) + 1):n test_meps &lt;- shuffled_meps[test_indices, ] explvars3 &lt;- c(&quot;gender&quot;, &quot;age&quot;, &quot;race&quot;, &quot;mpoor&quot;, &quot;anylimit&quot;, &quot;income&quot;, &quot;insure&quot;, &quot;usc&quot;) explvars4 &lt;- c(explvars3, &quot;region&quot;, &quot;educ&quot;, &quot;phstat&quot;, &quot;unemploy&quot;, &quot;managedcare&quot;) Sample_code `@sample_code` # Regress `logexpend` on the explanatory variables listed in `explvars3` meps_mlr3 &lt;- lm(logexpend ~ gender + age + phstat + anylimit + insure + mpoor, data = train_meps) # Predict expenditures (not logged) and summarize using the sum of absolute prediction errors. explvars3 &lt;- c(&quot;gender&quot;, &quot;age&quot;, &quot;phstat&quot;, &quot;anylimit&quot;, &quot;insure&quot;, &quot;mpoor&quot;) predict_meps3 &lt;- test_meps[,explvars3] predict_mlr3 &lt;- exp(predict(meps_mlr3, predict_meps3)) predict_err_mlr3 &lt;- test_meps$expendop - predict_mlr3 sape3 &lt;- sum(abs(predict_err_mlr3))/1000 # Regress `expendop` on all 13 explanatory variables meps_mlr4 &lt;- lm(___~ gender + age + race + region + educ + phstat + mpoor + anylimit + income + insure + usc + unemploy + managedcare, data = train_meps) # Predict expenditures and summarize using the sum of absolute prediction errors. predict_meps4 &lt;- test_meps[,explvars4] predict_mlr4 &lt;- predict(meps_mlr4, predict_meps4) predict_err_mlr4 &lt;- test_meps$expendop - predict_mlr4 sape4 &lt;- sum(abs(predict_err_mlr4))/1000 sape3;sape4 # Compare the predictions of the models graphically. par(mfrow = c(1, 2)) plot(predict_err_mlr4, predict_err_mlr3, xlab = &quot;Benchmark Predict Error&quot;, ylab = &quot;MLR Predict Error&quot;) plot(predict_mlr3, test_meps$expendop, xlab = &quot;MLR Predicts&quot;, ylab = &quot;Held Out Expends&quot;) Solution # Solution meps_mlr3 &lt;- lm(logexpend ~ gender + age + phstat + anylimit + insure + mpoor, data = train_meps) predict_meps3 &lt;- test_meps[,explvars3] explvars3 &lt;- c(&quot;gender&quot;, &quot;age&quot;, &quot;phstat&quot;, &quot;anylimit&quot;, &quot;insure&quot;, &quot;mpoor&quot;) predict_meps3 &lt;- test_meps[,explvars3] predict_mlr3 &lt;- exp(predict(meps_mlr3, predict_meps3)) predict_err_mlr3 &lt;- test_meps$expendop - predict_mlr3 sape3 &lt;- sum(abs(predict_err_mlr3))/1000 meps_mlr4 &lt;- lm(expendop ~ gender + age + race + region + educ + phstat + mpoor + anylimit + income + insure + usc + unemploy + managedcare, data = train_meps) predict_meps4 &lt;- test_meps[,explvars4] predict_mlr4 &lt;- predict(meps_mlr4, predict_meps4) predict_err_mlr4 &lt;- test_meps$expendop - predict_mlr4 sape4 &lt;- sum(abs(predict_err_mlr4))/1000 sape3;sape4 par(mfrow = c(1, 2)) plot(predict_err_mlr4, predict_err_mlr3, xlab = &quot;Benchmark Predict Error&quot;, ylab = &quot;MLR Predict Error&quot;) plot(predict_mlr3, test_meps$expendop, xlab = &quot;MLR Predicts&quot;, ylab = &quot;Held Out Expends&quot;) Submission correctness test (SCT) success_msg(“Excellent! We found that the model of log expenditures outperforms the benchmark that models expenditures, even when the out of sample criterion was in the original ‘dollar’ units. It is comoforting to know that a search for a good model does well when using different out of sample criteria.”) 5.2 What the modeling procedure tells us 5.2.1 Video (Exercise). What the modeling procedure tells us 5.2.1.1 Learning Objectives In this module, you learn how to: Interpret individual effects, based on their substantive and statistical significance Describe other purposes of regression modeling, including regression function for pricing, benchmarking studies, and predicting future observations. 5.2.1.2 Video Overheads Overhead A. Interpreting individual effects Substantive Effect Does a 1 unit change in \\(x\\) imply an economically meaningful change in \\(y\\)? Example: Looking at urban and rural claims experience, is there a big enough difference to warrant differentiating prices by location? Statistical Significance We have standards for deciding whether or not a variable is statistically significant. A “statistically significant effect” is the result of a regression coefficient that is large relative to its standard error. Statistical significance is driven by precision of \\(s\\), collinearity (\\(VIF\\)) and sample size Causal Effects If we change \\(x\\), would \\(y\\) change? Overhead B. Other Interpretations Regression function and pricing The regression function is \\(\\mathrm{E~}y = \\beta_0 + \\beta_1 x_1 + \\cdots +\\beta _k x_k\\). Think about expected claims as our baseline price for short-term insurance coverages. Benchmarking studies In studies of CEO’s salaries, who is making a lot (or a little), controlled for industry, years of experience and so forth? In studies of medical claims, who are the high-cost patients? Prediction A new patient comes in with a given set of characteristics, what can I say about his or her future medical claims? 5.2.2 MC Exercise. What the modeling procedure tells us Which of the following are not important when interpreting the effects of individual variables? A. Substantive significance B. Statistical significance C. The amount of effort that it took to gather the data and do the analysis D. Role of causality Which of the following is not a potential explanation for the lack of statistical significance of an explanatory variable? A. Large variation of the disturbance term B. High collinearity, so that the variable may be confounded with other variables. C. The coefficient of determination, \\(R^2\\), is not sufficiently large. Which of the following is not an important purpose of regression modeling? A. Pricing of risks such as insurance contracts B. Benchmarking studies, to compare an observation to others C. Prediction D. Keeping a computer occupied with work 5.3 The importance of variable selection 5.3.1 Video (Exercise). The importance of variable selection 5.3.1.1 Learning Objectives In this module, you learn how to: Describe the bias that can occur when omitting important variables Describe the principle of parsimony and reasons for adopting this approach 5.3.1.2 Video Overheads Overhead A. The importance of variable selection With too many or too few variables, \\(s\\) is too large an estimate of \\(\\sigma\\). Prediction intervals are too large Standard errors for the partial slopes are too large With too few or incorrect variables, we produce biased estimates of the slopes \\(\\beta\\). Thus, our predictions are biased and hence inaccurate. Overhead B. Example. Regression using one explanatory variable Too Many Variables The “true” model is \\(y_i = \\beta_0+ \\varepsilon_i\\) We mistakenly use \\(y_i = \\beta_0+ \\beta_1 x_i^* + \\varepsilon_i\\) The prediction at a generic level \\(x\\) is \\(b_0^* + b_1^* x\\). It is not to hard to confirm that \\(Bias = \\mathrm{E} (b_0^* + b_1^* x) - \\mathrm{E } y= 0\\). Too Few Variables The “true” model is \\(y_i = \\beta_0 + \\beta_1 x_i + \\varepsilon_i\\). We mistakenly use \\(y_i = \\beta_0^* \\varepsilon_i\\). Under the true model, \\(\\overline{y} = \\beta_0 + \\beta_1 \\overline{x} + \\overline{\\varepsilon}\\) Thus, the bias is \\[ Bias = \\text{E }\\bar{y} - \\text{E }(\\beta_0 + \\beta_1 x + \\varepsilon) \\\\ = \\text{E }(\\beta_0 + \\beta_1 \\bar{x}+\\bar{\\varepsilon})-(\\beta_0+\\beta_1 x)=\\beta_1 (\\bar{x}-x). \\] There is a persistent, long-term error in omitting the explanatory variable \\(x\\). Overhead C. Principle of parsimony The principle of parsimony, also known as Occam’s Razor, states that when there are several possible explanations for a phenomenon, use the simplest. A simpler explanation is easier to interpret. Simpler models, also known as ``more parsimonious’’ models, often do well on fitting out-of-sample data Extraneous variables can cause problems of collinearity, leading to difficulty in interpreting individual coefficients. In contrast, in a quote often attributed to Albert Einstein, we should use “the simplest model possible, but no simpler.” Omitting important variables can lead to biased results, a potentially serious error. Including extraneous variables decreases the degrees of freedom and increases the estimate of variability, typically of less concern in actuarial applications. 5.3.2 MC Exercise. Principle of parsimony Which of the following is true about under- and over-fitting a model? A. When we over-fit a model, estimates of regression coefficients are over-biased as is \\(s^2\\), the estimate of model variance \\(\\sigma^2\\). B. When we over-fit a model, estimates of regression coefficients are remain unbiased whereas \\(s^2\\), the estimate of model variance \\(\\sigma^2\\), is over-biased. C. When we over-fit a model, estimates of regression coefficients are remain under-biased as is \\(s^2\\), the estimate of model variance \\(\\sigma^2\\). D. When we under-fit a model, estimates of regression coefficients are remain unbiased whereas \\(s^2\\), the estimate of model variance \\(\\sigma^2\\), is over-biased. Which of the following is not true of Occam’s Razor? A. When there are several possible explanations for a phenomenon, use the simplest one. B. Simpler models are easier to interpret. C. Variables can be statistically significant but practically unimportant. D. Simpler models often do better for predicting out-of-sample data Overhead D. Course wrap up Thanks for watching "]
]
